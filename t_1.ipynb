{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.V = nn.Linear(args.hidden_dim, 1)\n",
    "        self.A = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
    "        return Q \n",
    "class DQN_Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DQN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.fc3 = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        Q = self.fc3(x)\n",
    "        return Q\n",
    "# class DQN_Net(nn.Module):\n",
    "#     def __init__(self, args, input_shape=(4, 84, 84)):\n",
    "#         super(DQN_Net, self).__init__()\n",
    "#         # 卷积层\n",
    "#         self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "#         # 计算卷积后的特征大小\n",
    "#         conv_out_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "#         # 全连接层\n",
    "#         self.fc1 = nn.Linear(conv_out_size, args.hidden_dim)\n",
    "#         self.fc2 = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "        \n",
    "#     def _get_conv_output(self, shape):\n",
    "#         bs = 1\n",
    "#         input = torch.rand(bs, *shape)\n",
    "#         output = self._forward_conv(input)\n",
    "#         return int(np.prod(output.size()))\n",
    "        \n",
    "#     def _forward_conv(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         return x\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # 检查输入维度，确保是4D张量 [batch, channels, height, width]\n",
    "#         if len(x.shape) == 3:\n",
    "#             x = x.unsqueeze(0)  # 添加批次维度\n",
    "            \n",
    "#         x = self._forward_conv(x)\n",
    "#         x = x.view(x.size(0), -1)  # 展平\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         return self.fc2(x)\n",
    "class Noisy_DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,sigma_init=0.5):\n",
    "        super(Noisy_DQN, self).__init__()\n",
    "        self.std_init = sigma_init\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.register_buffer('weight_epsilon', torch.Tensor(output_dim, input_dim))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.register_buffer('bias_epsilon', torch.Tensor(output_dim))\n",
    "        self.is_training = True\n",
    "        self.reset_parameters()\n",
    "        self.reset_noisy()\n",
    "    def forward(self,x):\n",
    "        if self.is_training:\n",
    "            #self.reset_noisy()\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(self.weight_epsilon)\n",
    "            bias = self.bias_mu + self.bias_sigma.mul(self.bias_epsilon)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.input_dim)\n",
    "        self.weight_mu.data.uniform_(-std, std)\n",
    "        self.weight_sigma.data.fill_(self.std_init/math.sqrt(self.input_dim))\n",
    "        self.bias_mu.data.uniform_(-std, std)\n",
    "        self.bias_sigma.data.fill_(self.std_init/math.sqrt(self.output_dim))\n",
    "    def reset_noisy(self):\n",
    "        epsilon_in = self._scale_noise(self.input_dim)\n",
    "        epsilon_out = self._scale_noise(self.output_dim)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x\n",
    "# class Distribution_DQN(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(Distribution_DQN, self).__init__()\n",
    "#         self.in_dim = args.state_dim\n",
    "#         self.out_dim = args.action_dim\n",
    "#         self.hidden_dim = args.hidden_dim\n",
    "#         self.num_atoms = args.num_atoms\n",
    "#         self.v_min = args.v_min\n",
    "#         self.v_max = args.v_max\n",
    "#         self.device = args.device\n",
    "#         self.num_actions = args.um_actions\n",
    "#         self.delta_z = (self.v_max - self.v_min) / (self.num_atoms - 1)\n",
    "#         self.fc1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "#         self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "#         self.fc3 = nn.Linear(self.hidden_dim, self.num_actions * self.num_atoms)\n",
    "#     def forward(self, x):\n",
    "#         dist = self.dist(x)\n",
    "#         support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "#         q_value = torch.sum(dist * support, dim=2)\n",
    "#         return q_value\n",
    "#     def dist(self,x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         x = x.view(-1, self.num_actions, self.num_atoms)\n",
    "#         dist = F.softmax(x, dim=-1)\n",
    "#         dist = dist.clamp(min=1e-3)\n",
    "#         return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NetWork,self).__init__()\n",
    "        self.in_dim = args.state_dim\n",
    "        self.out_dim = args.action_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.device = args.device\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        #self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.advantage_hidden = Noisy_DQN(self.hidden_dim, self.hidden_dim)\n",
    "        self.advantage = Noisy_DQN(self.hidden_dim, self.out_dim*self.num_atoms)\n",
    "        self.value_hidden = Noisy_DQN(self.hidden_dim, self.hidden_dim)\n",
    "        self.value = Noisy_DQN(self.hidden_dim, 1*self.num_atoms)\n",
    "    def forward(self,x):\n",
    "        dist = self.dist(x)\n",
    "        support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "        #print(dist.shape)\n",
    "        q_value = torch.sum(dist * support, dim=2)\n",
    "        return q_value\n",
    "       \n",
    "    def dist(self,x) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        advantage = F.relu(self.advantage_hidden(x))\n",
    "        advantage = self.advantage(advantage)\n",
    "        value = F.relu(self.value_hidden(x))\n",
    "        value = self.value(value)\n",
    "        advantage = advantage.view(-1, self.out_dim, self.num_atoms)\n",
    "        value = value.view(-1, 1, self.num_atoms)\n",
    "        q_value = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        dist = F.softmax(q_value, dim=-1)\n",
    "        dist = dist.clamp(min=1e-6)\n",
    "        return dist \n",
    "    def reset_noise(self):\n",
    "        self.advantage_hidden.reset_noisy()\n",
    "        self.advantage.reset_noisy()\n",
    "        self.value_hidden.reset_noisy()\n",
    "        self.value.reset_noisy()\n",
    "    def train(self, mode=True):\n",
    "        super(NetWork, self).train(mode)\n",
    "        self.is_training = mode\n",
    "    \n",
    "    # Explicitly set training mode for all Noisy layers\n",
    "        self.advantage_hidden.is_training = mode\n",
    "        self.advantage.is_training = mode\n",
    "        self.value_hidden.is_training = mode\n",
    "        self.value.is_training = mode\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_point = 0\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.tree_size = 2*buffer_size-1\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tree = np.zeros(self.tree_size)\n",
    "        self.data = np.zeros(buffer_size)\n",
    "    def update(self,data_idx,value):\n",
    "        tree_idx = data_idx+self.buffer_size-1\n",
    "        #print(f\"tree_idx: {tree_idx}, value: {value}, tree shape: {self.tree.shape}, tree dtype: {self.tree.dtype}\")\n",
    "        change = value-self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = value\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx-1)//2\n",
    "            self.tree[tree_idx] += change\n",
    "    def get_leaf(self,v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2*parent_idx+1\n",
    "            right_child_idx = left_child_idx+1\n",
    "            if left_child_idx>=self.tree_size:\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if v<=self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    v -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx-self.buffer_size+1\n",
    "        return data_idx,self.tree[leaf_idx]\n",
    "    def sample(self,n,beta):\n",
    "        batch_index = np.zeros(n, dtype=np.long)\n",
    "        IS_weight = torch.zeros(n, dtype=torch.float32)\n",
    "        #if not np.isfinite(self.total):\n",
    "            #Initialize with a small value instead\n",
    "            #total = 0.00001\n",
    "        #else:\n",
    "        total = self.total\n",
    "        pri_seg = total/n\n",
    "        #print(f\"total: {self.total}\")\n",
    "        tree_tensor = torch.tensor(self.tree[-self.buffer_size:], dtype=torch.float32)\n",
    "        min_prob = tree_tensor.min().item() / total\n",
    "        if min_prob == 0:\n",
    "            min_prob = 0.00001\n",
    "        prob_list = []\n",
    "        for i in range(n):\n",
    "            a,b= i*pri_seg,(i+1)*pri_seg\n",
    "            #a = max(0, min(a, total))\n",
    "            #b = max(a + 1e-6, min(b, total))\n",
    "            v = np.random.uniform(a,b)\n",
    "            data_idx,priority = self.get_leaf(v)\n",
    "            prob = priority/total\n",
    "            batch_index[i] = data_idx\n",
    "            prob_list.append(prob/min_prob)\n",
    "        IS_weight = torch.tensor(prob_list, dtype=torch.float32)\n",
    "        IS_weight = torch.pow(IS_weight, -beta)\n",
    "        return batch_index,IS_weight\n",
    "    def add(self,data_idx,value):\n",
    "        tree_idx = data_idx+self.buffer_size-1\n",
    "        self.data[self.data_point] = value\n",
    "        self.update(data_idx,value)\n",
    "        self.data_point += 1\n",
    "        if self.data_point>=self.buffer_size:\n",
    "            self.data_point = 0\n",
    "    @property\n",
    "    def max_priority(self):\n",
    "        return np.max(self.tree[-self.buffer_size:])\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# class Replaybuffer(object):\n",
    "#     def __init__(self,args):\n",
    "#         self.batch_size = args.batch_size\n",
    "#         self.buffer_capacity = args.buffer_capacity\n",
    "#         self.current_size = 0\n",
    "#         self.current_index = 0\n",
    "        \n",
    "#         self.buffer = {\n",
    "#             'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "#             'action':np.zeros((self.buffer_capacity,1)),\n",
    "#             'reward':np.zeros(self.buffer_capacity),\n",
    "#             'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "#             'done':np.zeros(self.buffer_capacity)\n",
    "#         }\n",
    "        \n",
    "#     def store_transition(self,state,action,reward,next_state,done):\n",
    "#         self.buffer['state'][self.current_index] = state\n",
    "#         self.buffer['action'][self.current_index] = action\n",
    "#         self.buffer['reward'][self.current_index] = reward\n",
    "#         self.buffer['next_state'][self.current_index] = next_state\n",
    "#         self.buffer['done'][self.current_index] = done\n",
    "#         self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "#         self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "#     def sample(self,total_step):\n",
    "        \n",
    "#         index = np.random.randint(0,self.current_size,size=self.batch_size)\n",
    "#         batch = {}\n",
    "#         for key in self.buffer.keys():\n",
    "#             if key=='action':\n",
    "#                 batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.long)\n",
    "#             else:\n",
    "#                 batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.float32)\n",
    "#         return batch,None,None\n",
    "# class N_Step_ReplayBuffer(object):\n",
    "#     def __init__(self,args):\n",
    "#             self.batch_size = args.batch_size\n",
    "#             self.buffer_capacity = args.buffer_capacity\n",
    "#             self.current_size = 0\n",
    "#             self.current_index = 0\n",
    "#             self.n_step = args.n_steps\n",
    "#             self.gamma = args.gamma\n",
    "#             self.n_step_deque = deque(maxlen=args.n_steps)\n",
    "#             self.buffer = {\n",
    "#                 'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "#                 'action':np.zeros((self.buffer_capacity,1)),\n",
    "#                 'reward':np.zeros(self.buffer_capacity),\n",
    "#                 'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "#                 'done':np.zeros(self.buffer_capacity)\n",
    "#             }\n",
    "#     def store_transition(self,state,action,reward,next_state,done):\n",
    "#         self.n_step_deque.append((state,action,reward,next_state,done))\n",
    "#         if len(self.n_step_deque)>=self.n_step:\n",
    "#             state, action, n_steps_reward, next_state, done = self.get_n_steps_transition()\n",
    "#             self.buffer['state'][self.current_index] = state\n",
    "#             self.buffer['action'][self.current_index] = action\n",
    "#             self.buffer['reward'][self.current_index] = n_steps_reward\n",
    "#             self.buffer['next_state'][self.current_index] = next_state\n",
    "#             self.buffer['done'][self.current_index] = done\n",
    "#             self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "#             self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "#     def get_n_steps_transition(self):\n",
    "#         state,action = self.n_step_deque[0][0],self.n_step_deque[0][1]\n",
    "#         next_state,done = self.n_step_deque[-1][3],self.n_step_deque[-1][4]\n",
    "#         n_steps_reward = 0\n",
    "#         for i in reversed(range(self.n_step)):\n",
    "#             r,_s,done = self.n_step_deque[i][2:]\n",
    "#             n_steps_reward = r + self.gamma*(1-done)*n_steps_reward\n",
    "#             if done:\n",
    "#                 next_state,done = _s,done\n",
    "#                 break\n",
    "#         return state,action,n_steps_reward,next_state,done\n",
    "#     def sample(self,total_step):\n",
    "#         index = np.random.randint(0,self.current_size,size=self.batch_size)\n",
    "#         batch = {}\n",
    "#         for key in self.buffer.keys():\n",
    "#             if key=='action':\n",
    "#                 batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.long)\n",
    "#             else:\n",
    "#                 batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.float32)\n",
    "#         return batch,None,None\n",
    "    \n",
    "# class Prioritized_ReplayBuffer(object):\n",
    "#     def __init__(self,args):\n",
    "#         self.batch_size = args.batch_size\n",
    "#         self.buffer_capacity = args.buffer_capacity\n",
    "#         self.current_size = 0\n",
    "#         self.current_index = 0\n",
    "#         self.alpha = args.alpha\n",
    "#         self.beta = args.beta_init\n",
    "#         self.sumtree = SumTree(self.buffer_capacity)\n",
    "#         self.beta_increment_per_sampling = args.beta_increment_per_sampling\n",
    "#         self.priority_eps = args.priority_eps\n",
    "#         self.max_train_steps = args.max_train_steps\n",
    "        \n",
    "#         self.tree = SumTree(self.buffer_capacity)\n",
    "        \n",
    "#         self.buffer = {\n",
    "#             'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "#             'action':np.zeros((self.buffer_capacity,1)),\n",
    "#             'reward':np.zeros(self.buffer_capacity),\n",
    "#             'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "#             'done':np.zeros(self.buffer_capacity)\n",
    "#         }\n",
    "#     def store_transition(self,state,action,reward,next_state,done):\n",
    "#         self.buffer['state'][self.current_index] = state\n",
    "#         self.buffer['action'][self.current_index] = action\n",
    "#         self.buffer['reward'][self.current_index] = reward\n",
    "#         self.buffer['next_state'][self.current_index] = next_state\n",
    "#         self.buffer['done'][self.current_index] = done\n",
    "#         max_priority = self.tree.max_priority if self.current_size else 1.0\n",
    "#         self.tree.add(self.current_index,max_priority)\n",
    "#         self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "#         self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "#     def sample(self,total_step):\n",
    "#         batch_index,IS_weight = self.tree.sample(self.batch_size,self.beta)\n",
    "#         batch = {}\n",
    "#         self.beta = self.beta + (1 - self.beta) * (total_step / self.max_train_steps)\n",
    "#         for key in self.buffer.keys():\n",
    "#             if key=='action':\n",
    "#                 batch[key] = torch.tensor(self.buffer[key][batch_index],dtype=torch.long)\n",
    "#             else:\n",
    "#                 batch[key] = torch.tensor(self.buffer[key][batch_index],dtype=torch.float32)\n",
    "#         return batch, batch_index, IS_weight \n",
    "#     def update_batch_priorities(self, batch_index, td_errors):  # 根据传入的td_error，更新batch_index所对应数据的priorities\n",
    "#         priorities = (np.abs(td_errors) + 0.01) ** self.alpha\n",
    "#         for index, priority in zip(batch_index, priorities):\n",
    "#             self.sum_tree.update(index, priority)\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, args):\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.alpha = args.alpha\n",
    "        self.beta_init = args.beta_init\n",
    "        self.beta = args.beta_init\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.sum_tree = SumTree(self.buffer_capacity)\n",
    "        self.n_steps = args.n_steps\n",
    "        self.n_steps_deque = deque(maxlen=self.n_steps)\n",
    "        self.buffer = {'state': np.zeros((self.buffer_capacity, args.state_dim)),\n",
    "                       'action': np.zeros((self.buffer_capacity, 1)),\n",
    "                       'reward': np.zeros(self.buffer_capacity),\n",
    "                       'next_state': np.zeros((self.buffer_capacity, args.state_dim)),\n",
    "                       'done': np.zeros(self.buffer_capacity),\n",
    "                       }\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.device = args.device\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.n_steps_deque.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_steps_deque) == self.n_steps:\n",
    "            state, action, n_steps_reward, next_state, done = self.get_n_steps_transition()\n",
    "            self.buffer['state'][self.current_index] = state\n",
    "            self.buffer['action'][self.current_index] = action\n",
    "            self.buffer['reward'][self.current_index] = n_steps_reward\n",
    "            self.buffer['next_state'][self.current_index] = next_state\n",
    "            self.buffer['done'][self.current_index] = done\n",
    "            max_priority = self.sum_tree.max_priority if self.current_size else 1.0\n",
    "            self.sum_tree.update(self.current_index, max_priority)\n",
    "            self.current_index = (self.current_index + 1) % self.buffer_capacity\n",
    "            self.current_size = min(self.current_size + 1, self.buffer_capacity)\n",
    "    def get_n_steps_transition(self):\n",
    "        state, action = self.n_steps_deque[0][0], self.n_steps_deque[0][1]\n",
    "        next_state, done = self.n_steps_deque[-1][3], self.n_steps_deque[-1][4]\n",
    "        n_steps_reward = 0\n",
    "        for i in reversed(range(self.n_steps)):\n",
    "            r, _s, done = self.n_steps_deque[i][2:]\n",
    "            n_steps_reward = r + self.gamma * (1 - done) * n_steps_reward\n",
    "            if done:\n",
    "                next_state, done = _s, done\n",
    "                break\n",
    "        return state, action, n_steps_reward, next_state, done\n",
    "    def sample(self, total_step):\n",
    "        batch_index, IS_weight = self.sum_tree.sample(self.batch_size, self.beta)\n",
    "        batch = {}\n",
    "        self.beta = self.beta_init + (1 - self.beta_init) * (total_step / self.max_train_steps)\n",
    "        for key in self.buffer.keys():\n",
    "            if key == 'action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index], dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index], dtype=torch.float32)\n",
    "        return batch, batch_index, IS_weight\n",
    "    def update_batch_priorities(self, batch_index, td_errors):  # 根据传入的td_error，更新batch_index所对应数据的priorities\n",
    "        values = (np.abs(td_errors) + 1e-6) ** self.alpha\n",
    "        #print(f\"values : {values}, values dtype: {values.dtype}\")\n",
    "    \n",
    "    # 批量计算权重 - 将priorities与support相乘并求和\n",
    "    # 每个priority乘以所有support元素并求和\n",
    "    # 使用矩阵乘法批量计算所有权重值\n",
    "        #values = torch.sum(self.support * priorities, dim=1)\n",
    "        #print(f\"values shape: {values.shape}, values dtype: {values.dtype}\")\n",
    "    # 批量应用clamp操作\n",
    "        # = values.clamp(min=1e-7)\n",
    "    \n",
    "    # 批量转换为CPU上的标量\n",
    "        #values_cpu = values.detach().cpu()\n",
    "\n",
    "    # 更新SumTree (这部分仍需循环，因为SumTree.update只能单个更新)\n",
    "        for index, value in zip(batch_index, values):\n",
    "            #print(f\"with value {value.item()}\")\n",
    "            self.sum_tree.update(index, value)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "class DQN(object):\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.action_dim = args.action_dim\n",
    "        self.state_dim = args.state_dim\n",
    "        self.lr = args.lr\n",
    "        self.dqn_net = NetWork(args).to(args.device)\n",
    "        self.target_net =deepcopy(self.dqn_net)\n",
    "        self.optimizer = torch.optim.Adam(self.dqn_net.parameters(),lr=args.lr)\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = args.target_update_freq\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.gamma = args.gamma\n",
    "        self.beta = args.beta_init\n",
    "        self.beta_increment_per_sampling = args.beta_increment_per_sampling\n",
    "        self.priority_eps = args.priority_eps\n",
    "        self.n_step = args.n_steps\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.batch_size = args.batch_size\n",
    "        self.replay_buffer = ReplayBuffer(args)\n",
    "        self.support = torch.linspace(self.v_min,self.v_max,self.num_atoms).to(args.device)\n",
    "    def choose_action(self,state,epsilon):\n",
    "        with torch.no_grad():\n",
    "        # 先将state移到正确设备上\n",
    "            state = torch.unsqueeze(torch.tensor(state,dtype=torch.float),0).to(self.args.device)\n",
    "        # 网络已在设备上，输出也在同一设备上\n",
    "            q = self.dqn_net(state)\n",
    "            if np.random.uniform() > epsilon:\n",
    "                action = q.argmax(dim=-1).item()\n",
    "            else:\n",
    "                action = np.random.randint(0, self.action_dim)\n",
    "        # 此处返回整数动作值即可，不需要转回张量\n",
    "        return action\n",
    "    def learn(self,total_step):\n",
    "        gamma = self.gamma**self.n_step\n",
    "        batch, idx, IS_weight = self.replay_buffer.sample(total_step)\n",
    "        IS_weight = torch.FloatTensor(IS_weight.reshape(-1,1)).to(self.args.device)\n",
    "        element_loss,td_error  = self.compute_loss(batch,total_step,gamma)\n",
    "        #print(f\"element_loss: {element_loss}\")\n",
    "        #print(element_loss.shape)\n",
    "        loss  = torch.mean(element_loss*IS_weight)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.dqn_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_for_prior = td_error.detach().cpu().numpy()  \n",
    "        new_priority = self.priority_eps + loss_for_prior\n",
    "        #print(f\"new_priority: {new_priority}\")\n",
    "        #print(f\"new_priority shape: {new_priority.shape}\")\n",
    "        #new_priority = torch.sum(new_priority*self.support, dim=1)\n",
    "        self.replay_buffer.update_batch_priorities(idx, new_priority)\n",
    "        self.dqn_net.reset_noise()\n",
    "        self.target_net.reset_noise()\n",
    "        \n",
    "        self.update_counter += 1\n",
    "        if self.update_counter%self.target_update_freq==0:\n",
    "            self.target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "        self.update_lr(total_step)\n",
    "    def update_lr(self,total_steps):\n",
    "        lr_now = 0.9 * self.lr * (1 - total_steps / self.max_train_steps) + 0.1 * self.lr\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr_now\n",
    "    def compute_loss(self,batch,total_step,gamma):\n",
    "       \n",
    "        state = torch.FloatTensor(batch[\"state\"]).to(self.args.device)\n",
    "        next_state = torch.FloatTensor(batch[\"next_state\"]).to(self.args.device)\n",
    "        action = torch.LongTensor(batch[\"action\"]).to(self.args.device)\n",
    "        reward = torch.FloatTensor(batch[\"reward\"].reshape(-1, 1)).to(self.args.device)\n",
    "        done = torch.FloatTensor(batch[\"done\"].reshape(-1, 1)).to(self.args.device)\n",
    "        delta_z = float(self.v_max - self.v_min)/(self.num_atoms-1)\n",
    "        action = action.squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn_net(next_state).argmax(1)\n",
    "            next_dist = self.target_net.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.num_atoms, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.num_atoms)\n",
    "                .to(self.args.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.args.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn_net.dist(state)\n",
    "        #print(f\"action shape: {action.shape}, action dtype: {action.dtype}\")\n",
    "        #print(f\"dist shape: {dist.shape}, dist dtype: {dist.dtype}\")\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        #print(f\"log_p shape: {log_p.shape}, log_p dtype: {log_p.dtype}\")\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "        #print(f\"elementwise_loss shape: {elementwise_loss.shape}, elementwise_loss dtype: {elementwise_loss.dtype}\")\n",
    "        Q_s_a = (dist[range(self.batch_size), action] * self.support).sum(dim=1)  # [batch_size]\n",
    "\n",
    "# 目标分布对应的期望 Q(s,a)，即投影后的分布\n",
    "        Q_target = (proj_dist * self.support).sum(dim=1)  # [batch_size]\n",
    "\n",
    "# 绝对 TD 误差，用于更新 PER\n",
    "        td_errors = (Q_target - Q_s_a).abs().detach()     # [batch_size]\n",
    "        #print(f\"td_errors shape: {td_errors.shape}, td_errors dtype: {td_errors.dtype}\")\n",
    "        return elementwise_loss,td_errors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env=CartPole-v1\n",
      "state_dim=4\n",
      "action_dim=2\n",
      "episode_limit=500\n",
      "device=cuda:0\n",
      "total_steps:0 \t evaluate_reward:10.0 \t epsilon：0\n",
      "total_steps:100 \t evaluate_reward:10.0 \t epsilon：0\n",
      "total_steps:200 \t evaluate_reward:9.666666666666666 \t epsilon：0\n",
      "total_steps:300 \t evaluate_reward:9.333333333333334 \t epsilon：0\n",
      "total_steps:400 \t evaluate_reward:11.333333333333334 \t epsilon：0\n",
      "total_steps:500 \t evaluate_reward:9.666666666666666 \t epsilon：0\n",
      "total_steps:600 \t evaluate_reward:25.666666666666668 \t epsilon：0\n",
      "total_steps:700 \t evaluate_reward:23.666666666666668 \t epsilon：0\n",
      "total_steps:800 \t evaluate_reward:24.0 \t epsilon：0\n",
      "total_steps:900 \t evaluate_reward:29.0 \t epsilon：0\n",
      "total_steps:1000 \t evaluate_reward:217.66666666666666 \t epsilon：0\n",
      "total_steps:1100 \t evaluate_reward:163.66666666666666 \t epsilon：0\n",
      "total_steps:1200 \t evaluate_reward:375.6666666666667 \t epsilon：0\n",
      "total_steps:1300 \t evaluate_reward:350.6666666666667 \t epsilon：0\n",
      "total_steps:1400 \t evaluate_reward:238.66666666666666 \t epsilon：0\n",
      "total_steps:1500 \t evaluate_reward:274.6666666666667 \t epsilon：0\n",
      "total_steps:1600 \t evaluate_reward:245.0 \t epsilon：0\n",
      "total_steps:1700 \t evaluate_reward:224.66666666666666 \t epsilon：0\n",
      "total_steps:1800 \t evaluate_reward:218.0 \t epsilon：0\n",
      "total_steps:1900 \t evaluate_reward:204.0 \t epsilon：0\n",
      "total_steps:2000 \t evaluate_reward:283.0 \t epsilon：0\n",
      "total_steps:2100 \t evaluate_reward:163.66666666666666 \t epsilon：0\n",
      "total_steps:2200 \t evaluate_reward:144.66666666666666 \t epsilon：0\n",
      "total_steps:2300 \t evaluate_reward:122.0 \t epsilon：0\n",
      "total_steps:2400 \t evaluate_reward:140.66666666666666 \t epsilon：0\n",
      "total_steps:2500 \t evaluate_reward:123.0 \t epsilon：0\n",
      "total_steps:2600 \t evaluate_reward:139.66666666666666 \t epsilon：0\n",
      "total_steps:2700 \t evaluate_reward:126.0 \t epsilon：0\n",
      "total_steps:2800 \t evaluate_reward:117.66666666666667 \t epsilon：0\n",
      "total_steps:2900 \t evaluate_reward:108.66666666666667 \t epsilon：0\n",
      "total_steps:3000 \t evaluate_reward:114.0 \t epsilon：0\n",
      "total_steps:3100 \t evaluate_reward:122.0 \t epsilon：0\n",
      "total_steps:3200 \t evaluate_reward:123.0 \t epsilon：0\n",
      "total_steps:3300 \t evaluate_reward:116.66666666666667 \t epsilon：0\n",
      "total_steps:3400 \t evaluate_reward:130.0 \t epsilon：0\n",
      "total_steps:3500 \t evaluate_reward:135.66666666666666 \t epsilon：0\n",
      "total_steps:3600 \t evaluate_reward:126.66666666666667 \t epsilon：0\n",
      "total_steps:3700 \t evaluate_reward:122.0 \t epsilon：0\n",
      "total_steps:3800 \t evaluate_reward:110.33333333333333 \t epsilon：0\n",
      "total_steps:3900 \t evaluate_reward:124.66666666666667 \t epsilon：0\n",
      "total_steps:4000 \t evaluate_reward:128.0 \t epsilon：0\n",
      "total_steps:4100 \t evaluate_reward:112.33333333333333 \t epsilon：0\n",
      "total_steps:4200 \t evaluate_reward:124.66666666666667 \t epsilon：0\n",
      "total_steps:4300 \t evaluate_reward:121.33333333333333 \t epsilon：0\n",
      "total_steps:4400 \t evaluate_reward:116.66666666666667 \t epsilon：0\n",
      "total_steps:4500 \t evaluate_reward:122.66666666666667 \t epsilon：0\n",
      "total_steps:4600 \t evaluate_reward:182.33333333333334 \t epsilon：0\n",
      "total_steps:4700 \t evaluate_reward:123.33333333333333 \t epsilon：0\n",
      "total_steps:4800 \t evaluate_reward:128.0 \t epsilon：0\n",
      "total_steps:4900 \t evaluate_reward:128.66666666666666 \t epsilon：0\n",
      "total_steps:5000 \t evaluate_reward:117.33333333333333 \t epsilon：0\n",
      "total_steps:5100 \t evaluate_reward:124.33333333333333 \t epsilon：0\n",
      "total_steps:5200 \t evaluate_reward:126.0 \t epsilon：0\n",
      "total_steps:5300 \t evaluate_reward:151.33333333333334 \t epsilon：0\n",
      "total_steps:5400 \t evaluate_reward:179.66666666666666 \t epsilon：0\n",
      "total_steps:5500 \t evaluate_reward:138.33333333333334 \t epsilon：0\n",
      "total_steps:5600 \t evaluate_reward:155.66666666666666 \t epsilon：0\n",
      "total_steps:5700 \t evaluate_reward:138.33333333333334 \t epsilon：0\n",
      "total_steps:5800 \t evaluate_reward:156.66666666666666 \t epsilon：0\n",
      "total_steps:5900 \t evaluate_reward:160.66666666666666 \t epsilon：0\n",
      "total_steps:6000 \t evaluate_reward:180.66666666666666 \t epsilon：0\n",
      "total_steps:6100 \t evaluate_reward:136.66666666666666 \t epsilon：0\n",
      "total_steps:6200 \t evaluate_reward:152.33333333333334 \t epsilon：0\n",
      "total_steps:6300 \t evaluate_reward:148.66666666666666 \t epsilon：0\n",
      "total_steps:6400 \t evaluate_reward:151.33333333333334 \t epsilon：0\n",
      "total_steps:6500 \t evaluate_reward:153.0 \t epsilon：0\n",
      "total_steps:6600 \t evaluate_reward:149.0 \t epsilon：0\n",
      "total_steps:6700 \t evaluate_reward:264.0 \t epsilon：0\n",
      "total_steps:6800 \t evaluate_reward:183.33333333333334 \t epsilon：0\n",
      "total_steps:6900 \t evaluate_reward:249.0 \t epsilon：0\n",
      "total_steps:7000 \t evaluate_reward:167.33333333333334 \t epsilon：0\n",
      "total_steps:7100 \t evaluate_reward:241.33333333333334 \t epsilon：0\n",
      "total_steps:7200 \t evaluate_reward:168.66666666666666 \t epsilon：0\n",
      "total_steps:7300 \t evaluate_reward:162.0 \t epsilon：0\n",
      "total_steps:7400 \t evaluate_reward:245.33333333333334 \t epsilon：0\n",
      "total_steps:7500 \t evaluate_reward:291.0 \t epsilon：0\n",
      "total_steps:7600 \t evaluate_reward:215.0 \t epsilon：0\n",
      "total_steps:7700 \t evaluate_reward:189.33333333333334 \t epsilon：0\n",
      "total_steps:7800 \t evaluate_reward:230.66666666666666 \t epsilon：0\n",
      "total_steps:7900 \t evaluate_reward:366.6666666666667 \t epsilon：0\n",
      "total_steps:8000 \t evaluate_reward:215.0 \t epsilon：0\n",
      "total_steps:8100 \t evaluate_reward:204.66666666666666 \t epsilon：0\n",
      "total_steps:8200 \t evaluate_reward:179.33333333333334 \t epsilon：0\n",
      "total_steps:8300 \t evaluate_reward:199.33333333333334 \t epsilon：0\n",
      "total_steps:8400 \t evaluate_reward:234.0 \t epsilon：0\n",
      "total_steps:8500 \t evaluate_reward:306.0 \t epsilon：0\n",
      "total_steps:8600 \t evaluate_reward:228.33333333333334 \t epsilon：0\n",
      "total_steps:8700 \t evaluate_reward:172.33333333333334 \t epsilon：0\n",
      "total_steps:8800 \t evaluate_reward:235.0 \t epsilon：0\n",
      "total_steps:8900 \t evaluate_reward:293.0 \t epsilon：0\n",
      "total_steps:9000 \t evaluate_reward:383.3333333333333 \t epsilon：0\n",
      "total_steps:9100 \t evaluate_reward:384.0 \t epsilon：0\n",
      "total_steps:9200 \t evaluate_reward:207.33333333333334 \t epsilon：0\n",
      "total_steps:9300 \t evaluate_reward:425.6666666666667 \t epsilon：0\n",
      "total_steps:9400 \t evaluate_reward:302.6666666666667 \t epsilon：0\n",
      "total_steps:9500 \t evaluate_reward:296.3333333333333 \t epsilon：0\n",
      "total_steps:9600 \t evaluate_reward:468.6666666666667 \t epsilon：0\n",
      "total_steps:9700 \t evaluate_reward:352.0 \t epsilon：0\n",
      "total_steps:9800 \t evaluate_reward:310.6666666666667 \t epsilon：0\n",
      "total_steps:9900 \t evaluate_reward:302.0 \t epsilon：0\n",
      "total_steps:10000 \t evaluate_reward:267.0 \t epsilon：0\n",
      "total_steps:10100 \t evaluate_reward:453.6666666666667 \t epsilon：0\n",
      "total_steps:10200 \t evaluate_reward:306.0 \t epsilon：0\n",
      "total_steps:10300 \t evaluate_reward:365.6666666666667 \t epsilon：0\n",
      "total_steps:10400 \t evaluate_reward:324.6666666666667 \t epsilon：0\n",
      "total_steps:10500 \t evaluate_reward:340.6666666666667 \t epsilon：0\n",
      "total_steps:10600 \t evaluate_reward:395.0 \t epsilon：0\n",
      "total_steps:10700 \t evaluate_reward:417.6666666666667 \t epsilon：0\n",
      "total_steps:10800 \t evaluate_reward:428.0 \t epsilon：0\n",
      "total_steps:10900 \t evaluate_reward:325.0 \t epsilon：0\n",
      "total_steps:11000 \t evaluate_reward:359.3333333333333 \t epsilon：0\n",
      "total_steps:11100 \t evaluate_reward:296.6666666666667 \t epsilon：0\n",
      "total_steps:11200 \t evaluate_reward:327.0 \t epsilon：0\n",
      "total_steps:11300 \t evaluate_reward:400.6666666666667 \t epsilon：0\n",
      "total_steps:11400 \t evaluate_reward:329.0 \t epsilon：0\n",
      "total_steps:11500 \t evaluate_reward:261.0 \t epsilon：0\n",
      "total_steps:11600 \t evaluate_reward:452.0 \t epsilon：0\n",
      "total_steps:11700 \t evaluate_reward:264.3333333333333 \t epsilon：0\n",
      "total_steps:11800 \t evaluate_reward:261.0 \t epsilon：0\n",
      "total_steps:11900 \t evaluate_reward:348.3333333333333 \t epsilon：0\n",
      "total_steps:12000 \t evaluate_reward:236.0 \t epsilon：0\n",
      "total_steps:12100 \t evaluate_reward:326.3333333333333 \t epsilon：0\n",
      "total_steps:12200 \t evaluate_reward:180.33333333333334 \t epsilon：0\n",
      "total_steps:12300 \t evaluate_reward:210.33333333333334 \t epsilon：0\n",
      "total_steps:12400 \t evaluate_reward:183.66666666666666 \t epsilon：0\n",
      "total_steps:12500 \t evaluate_reward:214.66666666666666 \t epsilon：0\n",
      "total_steps:12600 \t evaluate_reward:256.0 \t epsilon：0\n",
      "total_steps:12700 \t evaluate_reward:246.33333333333334 \t epsilon：0\n",
      "total_steps:12800 \t evaluate_reward:359.0 \t epsilon：0\n",
      "total_steps:12900 \t evaluate_reward:346.0 \t epsilon：0\n",
      "total_steps:13000 \t evaluate_reward:357.3333333333333 \t epsilon：0\n",
      "total_steps:13100 \t evaluate_reward:424.0 \t epsilon：0\n",
      "total_steps:13200 \t evaluate_reward:286.0 \t epsilon：0\n",
      "total_steps:13300 \t evaluate_reward:298.6666666666667 \t epsilon：0\n",
      "total_steps:13400 \t evaluate_reward:307.6666666666667 \t epsilon：0\n",
      "total_steps:13500 \t evaluate_reward:296.6666666666667 \t epsilon：0\n",
      "total_steps:13600 \t evaluate_reward:313.3333333333333 \t epsilon：0\n",
      "total_steps:13700 \t evaluate_reward:324.0 \t epsilon：0\n",
      "total_steps:13800 \t evaluate_reward:298.6666666666667 \t epsilon：0\n",
      "total_steps:13900 \t evaluate_reward:301.0 \t epsilon：0\n",
      "total_steps:14000 \t evaluate_reward:338.6666666666667 \t epsilon：0\n",
      "total_steps:14100 \t evaluate_reward:260.6666666666667 \t epsilon：0\n",
      "total_steps:14200 \t evaluate_reward:277.6666666666667 \t epsilon：0\n",
      "total_steps:14300 \t evaluate_reward:269.6666666666667 \t epsilon：0\n",
      "total_steps:14400 \t evaluate_reward:245.0 \t epsilon：0\n",
      "total_steps:14500 \t evaluate_reward:277.0 \t epsilon：0\n",
      "total_steps:14600 \t evaluate_reward:275.0 \t epsilon：0\n",
      "total_steps:14700 \t evaluate_reward:227.0 \t epsilon：0\n",
      "total_steps:14800 \t evaluate_reward:248.0 \t epsilon：0\n",
      "total_steps:14900 \t evaluate_reward:273.0 \t epsilon：0\n",
      "total_steps:15000 \t evaluate_reward:321.3333333333333 \t epsilon：0\n",
      "total_steps:15100 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:15200 \t evaluate_reward:474.6666666666667 \t epsilon：0\n",
      "total_steps:15300 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:15400 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:15500 \t evaluate_reward:445.3333333333333 \t epsilon：0\n",
      "total_steps:15600 \t evaluate_reward:223.33333333333334 \t epsilon：0\n",
      "total_steps:15700 \t evaluate_reward:249.33333333333334 \t epsilon：0\n",
      "total_steps:15800 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:15900 \t evaluate_reward:410.6666666666667 \t epsilon：0\n",
      "total_steps:16000 \t evaluate_reward:464.0 \t epsilon：0\n",
      "total_steps:16100 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:16200 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:16300 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:16400 \t evaluate_reward:139.33333333333334 \t epsilon：0\n",
      "total_steps:16500 \t evaluate_reward:114.0 \t epsilon：0\n",
      "total_steps:16600 \t evaluate_reward:186.66666666666666 \t epsilon：0\n",
      "total_steps:16700 \t evaluate_reward:275.0 \t epsilon：0\n",
      "total_steps:16800 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:16900 \t evaluate_reward:388.3333333333333 \t epsilon：0\n",
      "total_steps:17000 \t evaluate_reward:354.3333333333333 \t epsilon：0\n",
      "total_steps:17100 \t evaluate_reward:443.3333333333333 \t epsilon：0\n",
      "total_steps:17200 \t evaluate_reward:413.0 \t epsilon：0\n",
      "total_steps:17300 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:17400 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:17500 \t evaluate_reward:483.0 \t epsilon：0\n",
      "total_steps:17600 \t evaluate_reward:268.6666666666667 \t epsilon：0\n",
      "total_steps:17700 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:17800 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:17900 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18000 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18100 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18200 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18300 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18400 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18500 \t evaluate_reward:500.0 \t epsilon：0\n",
      "total_steps:18600 \t evaluate_reward:384.0 \t epsilon：0\n",
      "total_steps:18700 \t evaluate_reward:231.66666666666666 \t epsilon：0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[744]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]:\n\u001b[32m    159\u001b[39m     runner = Runner(args=args, env_name=env_names[env_index], number=\u001b[32m1\u001b[39m, seed=seed)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[744]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m state = next_state\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent.replay_buffer.current_size >= \u001b[38;5;28mself\u001b[39m.args.batch_size:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total_steps % \u001b[38;5;28mself\u001b[39m.args.evaluate_freq == \u001b[32m0\u001b[39m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m#print(\"total_steps:{} \\t evaluate_reward:{} \\t epsilon：{}\".format(self.total_steps, total_reward, self.epsilon))\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mself\u001b[39m.evaluate_policy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[743]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mDQN.learn\u001b[39m\u001b[34m(self, total_step)\u001b[39m\n\u001b[32m     45\u001b[39m loss  = torch.mean(element_loss*IS_weight)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.dqn_net.parameters(), \u001b[32m10\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/r_l/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/r_l/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/r_l/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "class Runner:\n",
    "    def __init__(self,args,env_name,number,seed):\n",
    "        self.args = args\n",
    "        self.env_name = env_name\n",
    "        self.number = number\n",
    "        self.seed = seed\n",
    "\n",
    "        self.env = gym.make(env_name)  # When training the policy, we need to build an environment\n",
    "        self.env_evaluate = gym.make(env_name)  # When evaluating the policy, we need to rebuild an environment\n",
    "        #self.env.seed(seed)\n",
    "        self.env.reset(seed=seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "        self.env_evaluate.reset(seed=seed)\n",
    "        self.env_evaluate.action_space.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        self.args.state_dim = self.env.observation_space.shape[0]\n",
    "        self.args.action_dim = self.env.action_space.n\n",
    "        self.args.episode_limit = self.env._max_episode_steps  # Maximum number of steps per episode\n",
    "        self.device = args.device\n",
    "        self.n_steps = args.n_steps\n",
    "        print(\"env={}\".format(self.env_name))\n",
    "        print(\"state_dim={}\".format(self.args.state_dim))\n",
    "        print(\"action_dim={}\".format(self.args.action_dim))\n",
    "        print(\"episode_limit={}\".format(self.args.episode_limit))\n",
    "        print(\"device={}\".format(self.device))\n",
    "        self.agent = DQN(args)\n",
    "\n",
    "        self.algorithm = 'DQN'\n",
    "        self.writer = SummaryWriter(log_dir='runs/DQN/{}_env_{}_number_{}_seed_{}'.format(self.algorithm, env_name, number, seed))\n",
    "\n",
    "        self.evaluate_num = 0  # Record the number of evaluations\n",
    "        self.evaluate_rewards = []  # Record the rewards during the evaluating\n",
    "        self.total_steps = 0  # Record the total steps during the training\n",
    "        self.epsilon = self.args.epsilon_init\n",
    "        self.epsilon_min = self.args.epsilon_min\n",
    "        self.epsilon_decay = (self.args.epsilon_init - self.args.epsilon_min) / self.args.epsilon_decay_steps\n",
    "        #self.replay_buffer = ReplayBuffer(args)  # Initialize the replay buffer\n",
    "        \n",
    "\n",
    "        self.beta = self.args.beta_init\n",
    "    # def make_env(self,env_name):\n",
    "    #     env = gym.make(env_name)\n",
    "    #     env = WarpFrame(env)\n",
    "    #     return env\n",
    "    # def my_make_env(self,env_name):\n",
    "    #     env = DummyVecEnv([lambda: self.make_env(env_name)])\n",
    "    #     env = VecFrameStack(env, n_stack=4)\n",
    "    #     if self.seed is not None:\n",
    "    #         env.seed(seed)\n",
    "    #     return env\n",
    "    def run(self, ):\n",
    "        self.evaluate_policy()\n",
    "        #total_reward = 0\n",
    "        while self.total_steps < self.args.max_train_steps:\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_steps = 0\n",
    "            #total_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, epsilon=self.epsilon)\n",
    "                next_state, reward, done,  truncated,_= self.env.step(action) \n",
    "                done = done or truncated\n",
    "                episode_steps += 1\n",
    "                self.total_steps += 1\n",
    "                #total_reward += reward\n",
    "                self.agent.replay_buffer.store_transition(state, action, reward, next_state, done)  # Store the transition\n",
    "                \n",
    "                \n",
    "                self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon - self.epsilon_decay > self.epsilon_min else self.epsilon_min\n",
    "\n",
    "                # When dead or win or reaching the max_episode_steps, done will be Ture, we need to distinguish them;\n",
    "                # terminal means dead or win,there is no next state s';\n",
    "                # but when reaching the max_episode_steps,there is a next state s' actually.\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "\n",
    "                if self.agent.replay_buffer.current_size >= self.args.batch_size:\n",
    "                    \n",
    "                    \n",
    "                    self.agent.learn(self.total_steps)\n",
    "\n",
    "                if self.total_steps % self.args.evaluate_freq == 0:\n",
    "                    #print(\"total_steps:{} \\t evaluate_reward:{} \\t epsilon：{}\".format(self.total_steps, total_reward, self.epsilon))\n",
    "                    self.evaluate_policy()\n",
    "        # Save reward\n",
    "        np.save('./data_train/{}_env_{}_number_{}_seed_{}.npy'.format(self.algorithm, self.env_name, self.number, self.seed), np.array(self.evaluate_rewards))\n",
    "\n",
    "    def evaluate_policy(self, ):\n",
    "        evaluate_reward = 0\n",
    "        self.agent.dqn_net.eval()\n",
    "        for _ in range(self.args.evaluate_times):\n",
    "            state = self.env_evaluate.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, epsilon=0)\n",
    "                next_state, reward, done,  truncated ,_= self.env_evaluate.step(action)\n",
    "                done = done or truncated\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            evaluate_reward += episode_reward\n",
    "        self.agent.dqn_net.train()\n",
    "        evaluate_reward /= self.args.evaluate_times\n",
    "        self.evaluate_rewards.append(evaluate_reward)\n",
    "        print(\"total_steps:{} \\t evaluate_reward:{} \\t epsilon：{}\".format(self.total_steps, evaluate_reward, self.epsilon))\n",
    "        self.writer.add_scalar('step_rewards_{}'.format(self.env_name), evaluate_reward, global_step=self.total_steps)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"Hyperparameter Setting for DQN\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda:0', help=\"device\")\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=int(4e5), help=\" Maximum number of training steps\")\n",
    "    parser.add_argument(\"--evaluate_freq\", type=float, default=1e2, help=\"Evaluate the policy every 'evaluate_freq' steps\")\n",
    "    parser.add_argument(\"--evaluate_times\", type=float, default=3, help=\"Evaluate times\")\n",
    "\n",
    "    parser.add_argument(\"--buffer_capacity\", type=int, default=int(1e5), help=\"The maximum replay-buffer capacity \")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=256, help=\"batch size\")\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=256, help=\"The number of neurons in hidden layers of the neural network\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate of actor\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor\")\n",
    "    parser.add_argument(\"--epsilon_init\", type=float, default=0, help=\"Initial epsilon\")\n",
    "    parser.add_argument(\"--epsilon_min\", type=float, default=0, help=\"Minimum epsilon\")\n",
    "    parser.add_argument(\"--epsilon_decay_steps\", type=int, default=int(1e5), help=\"How many steps before the epsilon decays to the minimum\")\n",
    "    parser.add_argument(\"--tau\", type=float, default=0.005, help=\"soft update the target network\")\n",
    "    parser.add_argument(\"--use_soft_update\", type=bool, default=False, help=\"Whether to use soft update\")\n",
    "    parser.add_argument(\"--target_update_freq\", type=int, default=100, help=\"Update frequency of the target network(hard update)\")\n",
    "    parser.add_argument(\"--n_steps\", type=int, default=1, help=\"n_steps\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.6, help=\"PER parameter\")\n",
    "    parser.add_argument(\"--beta_init\", type=float, default=0.4, help=\"Important sampling parameter in PER\")\n",
    "    parser.add_argument(\"--use_lr_decay\", type=bool, default=True, help=\"Learning rate Decay\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=10.0, help=\"Gradient clip\")\n",
    "    parser.add_argument(\"--num_atoms\", type=int, default=51, help=\"Number of atoms in distributional DQN\")\n",
    "    parser.add_argument(\"--v_min\", type=float, default=0, help=\"Minimum value of the support\")\n",
    "    parser.add_argument(\"--v_max\", type=float, default=500, help=\"Maximum value of the support\")\n",
    "    parser.add_argument(\"--priority_eps\", type=float, default=1e-6, help=\"Priority eps\")\n",
    "    parser.add_argument(\"--beta_increment_per_sampling\", type=float, default=0.001, help=\"Increment of beta per sampling\")\n",
    "\n",
    "    parser.add_argument(\"--use_double\", type=bool, default=True, help=\"Whether to use double Q-learning\")\n",
    "    parser.add_argument(\"--use_dueling\", type=bool, default=True, help=\"Whether to use dueling network\")\n",
    "    parser.add_argument(\"--use_noisy\", type=bool, default=True, help=\"Whether to use noisy network\")\n",
    "    parser.add_argument(\"--use_per\", type=bool, default=True, help=\"Whether to use PER\")\n",
    "    parser.add_argument(\"--use_n_steps\", type=bool, default=True, help=\"Whether to use n_steps Q-learning\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    #args = parser.parse_known_args()[0]\n",
    "    gym.register_envs(ale_py)\n",
    "    env_names = ['CartPole-v1', 'LunarLander-v3','PongNoFrameskip-v4']\n",
    "    env_index = 0\n",
    "    seed = 0\n",
    "    for seed in [0, 10, 100]:\n",
    "        runner = Runner(args=args, env_name=env_names[env_index], number=1, seed=seed)\n",
    "        runner.run()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r_l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
