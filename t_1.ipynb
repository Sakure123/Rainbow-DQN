{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import ale_py\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.V = nn.Linear(args.hidden_dim, 1)\n",
    "        self.A = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
    "        return Q \n",
    "class DQN_Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DQN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.fc3 = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        Q = self.fc3(x)\n",
    "        return Q\n",
    "\n",
    "class Noisy_DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,sigma_init=0.5):\n",
    "        super(Noisy_DQN, self).__init__()\n",
    "        self.std_init = sigma_init\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.register_buffer('weight_epsilon', torch.Tensor(output_dim, input_dim))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.register_buffer('bias_epsilon', torch.Tensor(output_dim))\n",
    "        self.is_training = True\n",
    "        self.reset_parameters()\n",
    "        self.reset_noisy()\n",
    "    def forward(self,x):\n",
    "        if self.is_training:\n",
    "            #self.reset_noisy()\n",
    "            weight = self.weight_mu + self.weight_sigma*self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma*self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.input_dim)\n",
    "        self.weight_mu.data.uniform_(-std, std)\n",
    "        self.weight_sigma.data.fill_(self.std_init/math.sqrt(self.input_dim))\n",
    "        self.bias_mu.data.uniform_(-std, std)\n",
    "        self.bias_sigma.data.fill_(self.std_init/math.sqrt(self.output_dim))\n",
    "    def reset_noisy(self):\n",
    "        epsilon_in = self._scale_noise(self.input_dim)\n",
    "        epsilon_out = self._scale_noise(self.output_dim)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x\n",
    "class Distribution_DQN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Distribution_DQN, self).__init__()\n",
    "        self.in_dim = args.state_dim\n",
    "        self.out_dim = args.action_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.device = args.device\n",
    "        self.num_actions = args.um_actions\n",
    "        self.delta_z = (self.v_max - self.v_min) / (self.num_atoms - 1)\n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, self.num_actions * self.num_atoms)\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "    def forward(self, x):\n",
    "        dist = self.dist(x)\n",
    "        q_value = torch.sum(dist * self.support, dim=2)\n",
    "        return q_value\n",
    "    def dist(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, self.num_actions, self.num_atoms)\n",
    "        dist = F.softmax(x, dim=-1)\n",
    "        dist = dist.clamp(min=1e-6)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NetWork,self).__init__()\n",
    "        self.in_dim = args.state_dim\n",
    "        self.out_dim = args.action_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        if args.use_distribution:\n",
    "            self.num_atoms = args.num_atoms\n",
    "        else:\n",
    "            self.num_atoms = 1\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.device = args.device\n",
    "        self.args = args\n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.fc3 = nn.Linear(args.hidden_dim, self.out_dim * self.num_atoms)\n",
    "        #self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        if args.use_noisy:\n",
    "            self.advantage_hidden = Noisy_DQN(self.hidden_dim, self.hidden_dim)\n",
    "            self.advantage = Noisy_DQN(self.hidden_dim, self.out_dim*self.num_atoms)\n",
    "            self.value_hidden = Noisy_DQN(self.hidden_dim, self.hidden_dim)\n",
    "            self.value = Noisy_DQN(self.hidden_dim, 1*self.num_atoms)\n",
    "        else:\n",
    "            self.advantage_hidden = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "            self.advantage = nn.Linear(self.hidden_dim, self.out_dim*self.num_atoms)\n",
    "            self.value_hidden = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "            self.value = nn.Linear(self.hidden_dim, 1*self.num_atoms)\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "    def forward(self,x):\n",
    "        q_value = self.dist(x)\n",
    "        if self.args.use_distribution:\n",
    "            q_value = torch.sum(q_value * self.support, dim=2)\n",
    "            return q_value\n",
    "        else:\n",
    "            q_value = q_value.squeeze(-1)\n",
    "            return q_value\n",
    "       \n",
    "    def dist(self,x) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.args.use_dueling:\n",
    "            advantage = F.relu(self.advantage_hidden(x))\n",
    "            advantage = self.advantage(advantage)\n",
    "            value = F.relu(self.value_hidden(x))\n",
    "            value = self.value(value)\n",
    "            advantage = advantage.view(-1, self.out_dim, self.num_atoms)\n",
    "            value = value.view(-1, 1, self.num_atoms)\n",
    "            q_value = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        else:\n",
    "            q_value = F.relu(self.fc2(x))\n",
    "            q_value = self.fc3(q_value)\n",
    "            q_value = q_value.view(-1, self.out_dim, self.num_atoms)\n",
    "        if self.args.use_distribution:\n",
    "            dist = F.softmax(q_value, dim=-1)\n",
    "            dist = dist.clamp(min=1e-6)\n",
    "            return dist\n",
    "        else:\n",
    "            return q_value \n",
    "    def reset_noise(self):\n",
    "        self.advantage_hidden.reset_noisy()\n",
    "        self.advantage.reset_noisy()\n",
    "        self.value_hidden.reset_noisy()\n",
    "        self.value.reset_noisy()\n",
    "    def train(self, mode=True):\n",
    "        super(NetWork, self).train(mode)\n",
    "        self.is_training = mode\n",
    "    \n",
    "    # Explicitly set training mode for all Noisy layers\n",
    "        self.advantage_hidden.is_training = mode\n",
    "        self.advantage.is_training = mode\n",
    "        self.value_hidden.is_training = mode\n",
    "        self.value.is_training = mode\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_point = 0\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.tree_size = 2*buffer_size-1\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tree = np.zeros(self.tree_size)\n",
    "        self.data = np.zeros(buffer_size)\n",
    "    def update(self,data_idx,value):\n",
    "        tree_idx = data_idx+self.buffer_size-1\n",
    "        change = value-self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = value\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx-1)//2\n",
    "            self.tree[tree_idx] += change\n",
    "    def get_leaf(self,v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2*parent_idx+1\n",
    "            right_child_idx = left_child_idx+1\n",
    "            if left_child_idx>=self.tree_size:\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if v<=self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    v -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx-self.buffer_size+1\n",
    "        return data_idx,self.tree[leaf_idx]\n",
    "    def sample(self,n,beta):\n",
    "        batch_index = np.zeros(n, dtype=np.long)\n",
    "        IS_weight = torch.zeros(n, dtype=torch.float32)\n",
    "       \n",
    "        total = self.total\n",
    "        pri_seg = total/n\n",
    "        \n",
    "        tree_tensor = torch.tensor(self.tree[-self.buffer_size:], dtype=torch.float32)\n",
    "        min_prob = tree_tensor.min().item() / total\n",
    "        if min_prob == 0:\n",
    "            min_prob = 0.00001\n",
    "        prob_list = []\n",
    "        for i in range(n):\n",
    "            a,b= i*pri_seg,(i+1)*pri_seg\n",
    "            \n",
    "            v = np.random.uniform(a,b)\n",
    "            data_idx,priority = self.get_leaf(v)\n",
    "            prob = priority/total\n",
    "            batch_index[i] = data_idx\n",
    "            prob_list.append(prob/min_prob)\n",
    "        IS_weight = torch.tensor(prob_list, dtype=torch.float32)\n",
    "        IS_weight = torch.pow(IS_weight, -beta)\n",
    "        return batch_index,IS_weight\n",
    "    @property\n",
    "    def max_priority(self):\n",
    "        return np.max(self.tree[-self.buffer_size:])\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Replaybuffer(object):\n",
    "    def __init__(self,args):\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        \n",
    "        self.buffer = {\n",
    "            'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'action':np.zeros((self.buffer_capacity,1)),\n",
    "            'reward':np.zeros(self.buffer_capacity),\n",
    "            'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'done':np.zeros(self.buffer_capacity)\n",
    "        }\n",
    "        \n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.buffer['state'][self.current_index] = state\n",
    "        self.buffer['action'][self.current_index] = action\n",
    "        self.buffer['reward'][self.current_index] = reward\n",
    "        self.buffer['next_state'][self.current_index] = next_state\n",
    "        self.buffer['done'][self.current_index] = done\n",
    "        self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "        self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "    def sample(self,total_step):\n",
    "        \n",
    "        index = np.random.randint(0,self.current_size,size=self.batch_size)\n",
    "        batch = {}\n",
    "        for key in self.buffer.keys():\n",
    "            if key=='action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.float32)\n",
    "        return batch,None,None\n",
    "class N_Step_ReplayBuffer(object):\n",
    "    def __init__(self,args):\n",
    "            self.batch_size = args.batch_size\n",
    "            self.buffer_capacity = args.buffer_capacity\n",
    "            self.current_size = 0\n",
    "            self.current_index = 0\n",
    "            self.n_step = args.n_steps\n",
    "            self.gamma = args.gamma\n",
    "            self.n_step_deque = deque(maxlen=args.n_steps)\n",
    "            self.buffer = {\n",
    "                'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "                'action':np.zeros((self.buffer_capacity,1)),\n",
    "                'reward':np.zeros(self.buffer_capacity),\n",
    "                'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "                'done':np.zeros(self.buffer_capacity)\n",
    "            }\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.n_step_deque.append((state,action,reward,next_state,done))\n",
    "        if len(self.n_step_deque)>=self.n_step:\n",
    "            state, action, n_steps_reward, next_state, done = self.get_n_steps_transition()\n",
    "            self.buffer['state'][self.current_index] = state\n",
    "            self.buffer['action'][self.current_index] = action\n",
    "            self.buffer['reward'][self.current_index] = n_steps_reward\n",
    "            self.buffer['next_state'][self.current_index] = next_state\n",
    "            self.buffer['done'][self.current_index] = done\n",
    "            self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "            self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "    def get_n_steps_transition(self):\n",
    "        state,action = self.n_step_deque[0][0],self.n_step_deque[0][1]\n",
    "        next_state,done = self.n_step_deque[-1][3],self.n_step_deque[-1][4]\n",
    "        n_steps_reward = 0\n",
    "        for i in reversed(range(self.n_step)):\n",
    "            r,_s,done = self.n_step_deque[i][2:]\n",
    "            n_steps_reward = r + self.gamma*(1-done)*n_steps_reward\n",
    "            if done:\n",
    "                next_state,done = _s,done\n",
    "                break\n",
    "        return state,action,n_steps_reward,next_state,done\n",
    "    def sample(self,total_step):\n",
    "        index = np.random.randint(0,self.current_size,size=self.batch_size)\n",
    "        batch = {}\n",
    "        for key in self.buffer.keys():\n",
    "            if key=='action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.float32)\n",
    "        return batch,None,None\n",
    "    \n",
    "class Prioritized_ReplayBuffer(object):\n",
    "    def __init__(self,args):\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        self.alpha = args.alpha\n",
    "        self.beta_init = args.beta_init\n",
    "        self.sumtree = SumTree(self.buffer_capacity)\n",
    "        self.beta_increment_per_sampling = args.beta_increment_per_sampling\n",
    "        self.priority_eps = args.priority_eps\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        \n",
    "        self.tree = SumTree(self.buffer_capacity)\n",
    "        \n",
    "        self.buffer = {\n",
    "            'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'action':np.zeros((self.buffer_capacity,1)),\n",
    "            'reward':np.zeros(self.buffer_capacity),\n",
    "            'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'done':np.zeros(self.buffer_capacity)\n",
    "        }\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.buffer['state'][self.current_index] = state\n",
    "        self.buffer['action'][self.current_index] = action\n",
    "        self.buffer['reward'][self.current_index] = reward\n",
    "        self.buffer['next_state'][self.current_index] = next_state\n",
    "        self.buffer['done'][self.current_index] = done\n",
    "        max_priority = self.tree.max_priority if self.current_size else 1\n",
    "        self.tree.add(self.current_index,max_priority)\n",
    "        self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "        self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "    def sample(self,total_step):\n",
    "        batch_index,IS_weight = self.tree.sample(self.batch_size,self.beta)\n",
    "        batch = {}\n",
    "        self.beta = self.beta_init + (1 - self.beta_init) * (total_step / self.max_train_steps)\n",
    "        for key in self.buffer.keys():\n",
    "            if key=='action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index],dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index],dtype=torch.float32)\n",
    "        return batch, batch_index, IS_weight \n",
    "    def update_batch_priorities(self, batch_index, td_errors):  # 根据传入的td_error，更新batch_index所对应数据的priorities\n",
    "        priorities = (np.abs(td_errors) + 1e-6) ** self.alpha\n",
    "        for index, priority in zip(batch_index, priorities):\n",
    "            self.sum_tree.update(index, priority)\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, args):\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.alpha = args.alpha\n",
    "        self.beta_init = args.beta_init\n",
    "        self.beta = args.beta_init\n",
    "        \n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.sum_tree = SumTree(self.buffer_capacity)\n",
    "        self.args = args\n",
    "        if args.use_n_steps:\n",
    "            self.n_steps = args.n_steps\n",
    "        else:\n",
    "            self.n_steps = 1\n",
    "        self.n_steps_deque = deque(maxlen=self.n_steps)\n",
    "        self.buffer = {'state': np.zeros((self.buffer_capacity, args.state_dim)),\n",
    "                       'action': np.zeros((self.buffer_capacity, 1)),\n",
    "                       'reward': np.zeros(self.buffer_capacity),\n",
    "                       'next_state': np.zeros((self.buffer_capacity, args.state_dim)),\n",
    "                       'done': np.zeros(self.buffer_capacity),\n",
    "                       }\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.device = args.device\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "        self.gamma = args.gamma**self.n_steps\n",
    "        \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.n_steps_deque.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_steps_deque) >= self.n_steps:\n",
    "            state, action, n_steps_reward, next_state, done = self.get_n_steps_transition()\n",
    "            self.buffer['state'][self.current_index] = state\n",
    "            self.buffer['action'][self.current_index] = action\n",
    "            self.buffer['reward'][self.current_index] = n_steps_reward\n",
    "            self.buffer['next_state'][self.current_index] = next_state\n",
    "            self.buffer['done'][self.current_index] = done\n",
    "            max_priority = self.sum_tree.max_priority if self.current_size else 1\n",
    "            self.sum_tree.update(self.current_index, max_priority)\n",
    "            self.current_index = (self.current_index + 1) % self.buffer_capacity\n",
    "            self.current_size = min(self.current_size + 1, self.buffer_capacity)\n",
    "    def get_n_steps_transition(self):\n",
    "        state, action = self.n_steps_deque[0][0], self.n_steps_deque[0][1]\n",
    "        next_state, done = self.n_steps_deque[-1][3], self.n_steps_deque[-1][4]\n",
    "        n_steps_reward = 0\n",
    "        for i in reversed(range(self.n_steps)):\n",
    "            r, _s, done = self.n_steps_deque[i][2:]\n",
    "            n_steps_reward = r + self.gamma * (1 - done) * n_steps_reward\n",
    "            if done:\n",
    "                next_state, done = _s, done\n",
    "                break\n",
    "        return state, action, n_steps_reward, next_state, done\n",
    "    def sample(self, total_step):\n",
    "        if self.args.use_per:\n",
    "            batch_index, IS_weight = self.sum_tree.sample(self.batch_size, self.beta)\n",
    "        else:\n",
    "            batch_index = np.random.randint(0, self.current_size, size=self.batch_size)\n",
    "        batch = {}\n",
    "        self.beta = self.beta_init + (1 - self.beta_init) * (total_step / self.max_train_steps)\n",
    "        for key in self.buffer.keys():\n",
    "            if key == 'action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index], dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index], dtype=torch.float32)\n",
    "        if self.args.use_per:\n",
    "            return batch, batch_index, IS_weight\n",
    "        else:\n",
    "            return batch, None, None\n",
    "    def update_batch_priorities(self, batch_index, td_errors): \n",
    "        values = (np.abs(td_errors) + 1e-6) ** self.alpha\n",
    "        \n",
    "        for index, value in zip(batch_index, values):\n",
    "               \n",
    "            self.sum_tree.update(index, value)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "class DQN(object):\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.action_dim = args.action_dim\n",
    "        self.state_dim = args.state_dim\n",
    "        self.lr = args.lr\n",
    "        self.dqn_net = NetWork(args).to(args.device)\n",
    "        self.target_net =deepcopy(self.dqn_net)\n",
    "        self.optimizer = torch.optim.Adam(self.dqn_net.parameters(),lr=args.lr)\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = args.target_update_freq\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.gamma = args.gamma\n",
    "        self.tau = args.tau\n",
    "        self.beta = args.beta_init\n",
    "        self.beta_increment_per_sampling = args.beta_increment_per_sampling\n",
    "        self.priority_eps = args.priority_eps\n",
    "        if args.use_n_steps:\n",
    "            self.n_step = args.n_steps\n",
    "        else:\n",
    "            self.n_step = 1\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.batch_size = args.batch_size\n",
    "        self.replay_buffer = ReplayBuffer(args)\n",
    "        self.support = torch.linspace(self.v_min,self.v_max,self.num_atoms).to(args.device)\n",
    "    def choose_action(self,state,epsilon):\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            state = torch.unsqueeze(torch.tensor(state,dtype=torch.float),0).to(self.args.device)\n",
    "       \n",
    "            q = self.dqn_net(state)\n",
    "            if np.random.uniform() > epsilon:\n",
    "                action = q.argmax(dim=-1).item()\n",
    "            else:\n",
    "                action = np.random.randint(0, self.action_dim)\n",
    "        return action\n",
    "    def learn(self,total_step):\n",
    "        if self.args.use_noisy:\n",
    "            self.dqn_net.reset_noise()\n",
    "            self.target_net.reset_noise()\n",
    "        gamma = self.gamma**self.n_step\n",
    "        if self.args.use_per:\n",
    "            batch, idx, IS_weight = self.replay_buffer.sample(total_step)\n",
    "            IS_weight = torch.FloatTensor(IS_weight.reshape(-1,1)).to(self.args.device)\n",
    "        else:\n",
    "            batch, _, _ = self.replay_buffer.sample(total_step)\n",
    "            IS_weight = 1\n",
    "        element_loss,td_error  = self.compute_loss(batch,total_step,gamma)\n",
    "        loss  = torch.mean(element_loss*IS_weight)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.dqn_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        if self.args.use_per:\n",
    "            loss_for_prior = td_error.detach().cpu().numpy()  \n",
    "            new_priority = self.priority_eps + loss_for_prior\n",
    "            self.replay_buffer.update_batch_priorities(idx, new_priority)\n",
    "        \n",
    "        # self.update_counter += 1\n",
    "        # if self.update_counter % self.target_update_freq == 0:\n",
    "        for param, target_param in zip(self.dqn_net.parameters(), self.target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        self.update_lr(total_step)\n",
    "    def update_lr(self,total_steps):\n",
    "        lr_now = 0.9 * self.lr * (1 - total_steps / 1e5) + 0.1 * self.lr\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr_now\n",
    "    def compute_loss(self,batch,total_step,gamma):\n",
    "       \n",
    "        state = torch.FloatTensor(batch[\"state\"]).to(self.args.device)\n",
    "        next_state = torch.FloatTensor(batch[\"next_state\"]).to(self.args.device)\n",
    "        action = torch.LongTensor(batch[\"action\"]).to(self.args.device)\n",
    "        reward = torch.FloatTensor(batch[\"reward\"].reshape(-1, 1)).to(self.args.device)\n",
    "        done = torch.FloatTensor(batch[\"done\"].reshape(-1, 1)).to(self.args.device)\n",
    "        delta_z = float(self.v_max - self.v_min)/(self.num_atoms-1)\n",
    "        action = action.squeeze(1)\n",
    "        if not self.args.use_distribution:\n",
    "            with torch.no_grad():\n",
    "                next_q_values        = self.target_net(next_state)     # (B, A)\n",
    "                max_next_q_values,_ = next_q_values.max(dim=1, keepdim=True)  # (B, 1)\n",
    "                target_q_value       = reward + (1 - done) * gamma * max_next_q_values  # (B,1)\n",
    "            q_values = self.dqn_net(state)                    # (B, A)\n",
    "            q_value  = q_values.gather(1, action.unsqueeze(1))           # (B, 1)\n",
    "           \n",
    "            loss = F.mse_loss(q_value, target_q_value)\n",
    "            td_error = (q_value - target_q_value).abs().detach()\n",
    "            return loss, td_error\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn_net(next_state).argmax(1)\n",
    "            next_dist = self.target_net.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.num_atoms, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.num_atoms)\n",
    "                .to(self.args.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.args.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn_net.dist(state)\n",
    "       \n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        \n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "       \n",
    "        Q_s_a = (dist[range(self.batch_size), action] * self.support).sum(dim=1)  \n",
    "\n",
    "\n",
    "        Q_target = (proj_dist * self.support).sum(dim=1)  # [batch_size]\n",
    "\n",
    "\n",
    "        td_errors = (Q_target - Q_s_a).abs().detach()     # [batch_size]\n",
    "        #print(f\"td_errors shape: {td_errors.shape}, td_errors dtype: {td_errors.dtype}\")\n",
    "        return elementwise_loss,td_errors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "class Runner:\n",
    "    def __init__(self,args,env_name,number,seed):\n",
    "        self.args = args\n",
    "        self.env_name = env_name\n",
    "        self.number = number\n",
    "        self.seed = seed\n",
    "\n",
    "        self.env = gym.make(env_name)  # When training the policy, we need to build an environment\n",
    "        self.env_evaluate = gym.make(env_name)  # When evaluating the policy, we need to rebuild an environment\n",
    "        #self.env.seed(seed)\n",
    "        self.env.reset(seed=seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "        self.env_evaluate.reset(seed=seed)\n",
    "        self.env_evaluate.action_space.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed=seed)\n",
    "        torch.cuda.manual_seed_all(seed=seed)\n",
    "\n",
    "        self.args.state_dim = self.env.observation_space.shape[0]\n",
    "        self.args.action_dim = self.env.action_space.n\n",
    "        self.args.episode_limit = self.env._max_episode_steps  # Maximum number of steps per episode\n",
    "        self.device = args.device\n",
    "        self.n_steps = args.n_steps\n",
    "        print(\"env={}\".format(self.env_name))\n",
    "        print(\"state_dim={}\".format(self.args.state_dim))\n",
    "        print(\"action_dim={}\".format(self.args.action_dim))\n",
    "        print(\"episode_limit={}\".format(self.args.episode_limit))\n",
    "        print(\"device={}\".format(self.device))\n",
    "        self.agent = DQN(args)\n",
    "\n",
    "        self.algorithm = 'Rainbow-DQN'\n",
    "        self.writer = SummaryWriter(log_dir='runs/DQN/{}_env_{}_number_{}_seed_{}'.format(self.algorithm, env_name, number, seed))\n",
    "\n",
    "        self.evaluate_num = 0  # Record the number of evaluations\n",
    "        self.evaluate_rewards = []  # Record the rewards during the evaluating\n",
    "        self.total_steps = 0  # Record the total steps during the training\n",
    "        if args.use_noisy:\n",
    "            self.epsilon = 0.1\n",
    "            self.epsilon_min = 0.01\n",
    "        else:\n",
    "            self.epsilon = 0.5\n",
    "            self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = (0.5 - 0.1) / self.args.epsilon_decay_steps\n",
    "        #self.replay_buffer = ReplayBuffer(args)  # Initialize the replay buffer\n",
    "        \n",
    "\n",
    "        self.beta = self.args.beta_init\n",
    "        self.writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(self.args).items()])),\n",
    "    )\n",
    "    def run(self, ):\n",
    "        self.evaluate_policy()\n",
    "        #total_reward = 0\n",
    "        while self.total_steps < self.args.max_train_steps:\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_steps = 0\n",
    "            #total_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, epsilon=self.epsilon)\n",
    "                next_state, reward, done,  truncated,_= self.env.step(action) \n",
    "                done = done or truncated\n",
    "                episode_steps += 1\n",
    "                self.total_steps += 1\n",
    "                #total_reward += reward\n",
    "                self.agent.replay_buffer.store_transition(state, action, reward, next_state, done)  # Store the transition\n",
    "                \n",
    "                \n",
    "                self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon - self.epsilon_decay > self.epsilon_min else self.epsilon_min\n",
    "\n",
    "               \n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if self.agent.replay_buffer.current_size >= self.args.batch_size:\n",
    "                    \n",
    "                    \n",
    "                        self.agent.learn(self.total_steps)\n",
    "\n",
    "                if self.total_steps % self.args.evaluate_freq == 0:\n",
    "                    \n",
    "                    self.evaluate_policy()\n",
    "        # Save reward\n",
    "        # np.save('./data_train/{}_env_{}_number_{}_seed_{}.npy'.format(self.algorithm, self.env_name, self.number, self.seed), np.array(self.evaluate_rewards))\n",
    "        \n",
    "\n",
    "    def evaluate_policy(self, ):\n",
    "        evaluate_reward = 0\n",
    "        self.agent.dqn_net.eval()\n",
    "        for _ in range(self.args.evaluate_times):\n",
    "            state = self.env_evaluate.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, epsilon=0)\n",
    "                next_state, reward, done,  truncated ,_= self.env_evaluate.step(action)\n",
    "                done = done or truncated\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            evaluate_reward += episode_reward\n",
    "        self.agent.dqn_net.train()\n",
    "        evaluate_reward /= self.args.evaluate_times\n",
    "        self.evaluate_rewards.append(evaluate_reward)\n",
    "        print(\"total_steps:{} \\t evaluate_reward:{} \\t epsilon：{}\".format(self.total_steps, evaluate_reward, self.epsilon))\n",
    "        self.writer.add_scalar('{}'.format(self.env_name), evaluate_reward, global_step=self.total_steps)\n",
    "        #wandb.log({\"episode\": self.total_steps, \"reward\": evaluate_reward})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"Hyperparameter Setting for DQN\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda:0', help=\"device\")\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=int(5e4), help=\" Maximum number of training steps\")\n",
    "    parser.add_argument(\"--evaluate_freq\", type=float, default=1e2, help=\"Evaluate the policy every 'evaluate_freq' steps\")\n",
    "    parser.add_argument(\"--evaluate_times\", type=float, default=3, help=\"Evaluate times\")\n",
    "\n",
    "    parser.add_argument(\"--buffer_capacity\", type=int, default=int(1e5), help=\"The maximum replay-buffer capacity \")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size\")\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=256, help=\"The number of neurons in hidden layers of the neural network\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate of actor\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor\")\n",
    "    parser.add_argument(\"--epsilon_init\", type=float, default=0, help=\"Initial epsilon\")\n",
    "    parser.add_argument(\"--epsilon_min\", type=float, default=0, help=\"Minimum epsilon\")\n",
    "    parser.add_argument(\"--epsilon_decay_steps\", type=int, default=int(1e5), help=\"How many steps before the epsilon decays to the minimum\")\n",
    "    parser.add_argument(\"--tau\", type=float, default=0.005, help=\"soft update the target network\")\n",
    "    parser.add_argument(\"--target_update_freq\", type=int, default=500, help=\"Update frequency of the target network(hard update)\")\n",
    "    parser.add_argument(\"--n_steps\", type=int, default=3, help=\"n_steps\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.6, help=\"PER parameter\")\n",
    "    parser.add_argument(\"--beta_init\", type=float, default=0.4, help=\"Important sampling parameter in PER\")\n",
    "    parser.add_argument(\"--use_lr_decay\", type=bool, default=True, help=\"Learning rate Decay\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=10.0, help=\"Gradient clip\")\n",
    "    parser.add_argument(\"--num_atoms\", type=int, default=51, help=\"Number of atoms in distributional DQN\")\n",
    "    parser.add_argument(\"--v_min\", type=float, default=0, help=\"Minimum value of the support\")\n",
    "    parser.add_argument(\"--v_max\", type=float, default=500, help=\"Maximum value of the support\")\n",
    "    parser.add_argument(\"--priority_eps\", type=float, default=1e-6, help=\"Priority eps\")\n",
    "    parser.add_argument(\"--beta_increment_per_sampling\", type=float, default=0.001, help=\"Increment of beta per sampling\")\n",
    "\n",
    "    parser.add_argument(\"--use_double\", type=bool, default=True, help=\"Whether to use double Q-learning\")\n",
    "    parser.add_argument(\"--use_dueling\", type=bool, default=True, help=\"Whether to use dueling network\")\n",
    "    parser.add_argument(\"--use_noisy\", type=bool, default=True, help=\"Whether to use noisy network\")\n",
    "    parser.add_argument(\"--use_per\", type=bool, default=True, help=\"Whether to use PER\")\n",
    "    parser.add_argument(\"--use_n_steps\", type=bool, default=True, help=\"Whether to use n_steps Q-learning\")\n",
    "    parser.add_argument(\"--use_distribution\", type=bool, default=True, help=\"Whether to use distributional DQN\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    #args = parser.parse_known_args()[0]\n",
    "    group_name = \"RL_experiment_auto\"\n",
    "    env_names = ['CartPole-v1', 'LunarLander-v3','PongNoFrameskip-v4']\n",
    "    env_index = 0\n",
    "    seed = 20\n",
    "    wandb.init(\n",
    "            project=group_name,\n",
    "            sync_tensorboard=True,\n",
    "            name=\"Rainbow\",\n",
    "            #save_code=True,\n",
    "            config=vars(args),\n",
    "        )\n",
    "        #run = wandb.init(project=\"Rainbow\", group=group_name, config={\"seed\": seed}, reinit=True)\n",
    "    runner = Runner(args=args, env_name=env_names[env_index], number=1, seed=seed)\n",
    "    runner.run()\n",
    "        #run.finish()\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r_l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
