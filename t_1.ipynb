{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.V = nn.Linear(args.hidden_dim, 1)\n",
    "        self.A = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
    "        return Q \n",
    "class DQN_Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DQN_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(args.state_dim, args.hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.hidden_dim, args.hidden_dim)\n",
    "        self.fc3 = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        Q = self.fc3(x)\n",
    "        return Q\n",
    "# class DQN_Net(nn.Module):\n",
    "#     def __init__(self, args, input_shape=(4, 84, 84)):\n",
    "#         super(DQN_Net, self).__init__()\n",
    "#         # 卷积层\n",
    "#         self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "#         # 计算卷积后的特征大小\n",
    "#         conv_out_size = self._get_conv_output(input_shape)\n",
    "        \n",
    "#         # 全连接层\n",
    "#         self.fc1 = nn.Linear(conv_out_size, args.hidden_dim)\n",
    "#         self.fc2 = nn.Linear(args.hidden_dim, args.action_dim)\n",
    "        \n",
    "#     def _get_conv_output(self, shape):\n",
    "#         bs = 1\n",
    "#         input = torch.rand(bs, *shape)\n",
    "#         output = self._forward_conv(input)\n",
    "#         return int(np.prod(output.size()))\n",
    "        \n",
    "#     def _forward_conv(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         return x\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # 检查输入维度，确保是4D张量 [batch, channels, height, width]\n",
    "#         if len(x.shape) == 3:\n",
    "#             x = x.unsqueeze(0)  # 添加批次维度\n",
    "            \n",
    "#         x = self._forward_conv(x)\n",
    "#         x = x.view(x.size(0), -1)  # 展平\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         return self.fc2(x)\n",
    "class Noisy_DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,sigma_init=0.5):\n",
    "        super(Noisy_DQN, self).__init__()\n",
    "        self.std_init = sigma_init\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(output_dim, input_dim))\n",
    "        self.register_buffer('weight_epsilon', torch.Tensor(output_dim, input_dim))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.register_buffer('bias_epsilon', torch.Tensor(output_dim))\n",
    "        self.is_training = True\n",
    "        self.reset_parameters()\n",
    "        self.reset_noisy()\n",
    "    def forward(self,x):\n",
    "        if self.is_training:\n",
    "            #self.reset_noisy()\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(self.weight_epsilon)\n",
    "            bias = self.bias_mu + self.bias_sigma.mul(self.bias_epsilon)\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return F.linear(x, weight, bias)\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.input_dim)\n",
    "        self.weight_mu.data.uniform_(-std, std)\n",
    "        self.weight_sigma.data.fill_(self.std_init/math.sqrt(self.input_dim))\n",
    "        self.bias_mu.data.uniform_(-std, std)\n",
    "        self.bias_sigma.data.fill_(self.std_init/math.sqrt(self.output_dim))\n",
    "    def reset_noisy(self):\n",
    "        epsilon_in = self._scale_noise(self.input_dim)\n",
    "        epsilon_out = self._scale_noise(self.output_dim)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x\n",
    "class Distribution_DQN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Distribution_DQN, self).__init__()\n",
    "        self.in_dim = args.state_dim\n",
    "        self.out_dim = args.action_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.device = args.device\n",
    "        self.num_actions = args.um_actions\n",
    "        self.delta_z = (self.v_max - self.v_min) / (self.num_atoms - 1)\n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fc3 = nn.Linear(self.hidden_dim, self.num_actions * self.num_atoms)\n",
    "    def forward(self, x):\n",
    "        dist = self.dist(x)\n",
    "        support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "        q_value = torch.sum(dist * support, dim=2)\n",
    "        return q_value\n",
    "    def dist(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, self.num_actions, self.num_atoms)\n",
    "        dist = F.softmax(x, dim=-1)\n",
    "        dist = dist.clamp(min=1e-3)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NetWork,self).__init__()\n",
    "        self.in_dim = args.state_dim\n",
    "        self.out_dim = args.action_dim\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.device = args.device\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hidden_dim)\n",
    "        #self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.advantage_hidden = Noisy_DQN(self.hidden_dim, self.hidden_dim)\n",
    "        self.advantage = Noisy_DQN(self.hidden_dim, self.out_dim*self.num_atoms)\n",
    "        self.value_hidden = Noisy_DQN(self.hidden_dim, self.hidden_dim)\n",
    "        self.value = Noisy_DQN(self.hidden_dim, 1*self.num_atoms)\n",
    "    def forward(self,x):\n",
    "        dist = self.dist(x)\n",
    "        support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "        #print(dist.shape)\n",
    "        q_value = torch.sum(dist * support, dim=2)\n",
    "        return q_value\n",
    "       \n",
    "    def dist(self,x) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        advantage = F.relu(self.advantage_hidden(x))\n",
    "        advantage = self.advantage(advantage)\n",
    "        value = F.relu(self.value_hidden(x))\n",
    "        value = self.value(value)\n",
    "        advantage = advantage.view(-1, self.out_dim, self.num_atoms)\n",
    "        value = value.view(-1, 1, self.num_atoms)\n",
    "        q_value = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        dist = F.softmax(q_value, dim=-1)\n",
    "        dist = dist.clamp(min=1e-6)\n",
    "        return dist \n",
    "    def reset_noise(self):\n",
    "        self.advantage_hidden.reset_noisy()\n",
    "        self.advantage.reset_noisy()\n",
    "        self.value_hidden.reset_noisy()\n",
    "        self.value.reset_noisy()\n",
    "    def train(self, mode=True):\n",
    "        super(NetWork, self).train(mode)\n",
    "        self.is_training = mode\n",
    "    \n",
    "    # Explicitly set training mode for all Noisy layers\n",
    "        self.advantage_hidden.is_training = mode\n",
    "        self.advantage.is_training = mode\n",
    "        self.value_hidden.is_training = mode\n",
    "        self.value.is_training = mode\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_point = 0\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.tree_size = 2*buffer_size-1\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tree = np.zeros(self.tree_size)\n",
    "        self.data = np.zeros(buffer_size)\n",
    "    def update(self,data_idx,value):\n",
    "        tree_idx = data_idx+self.buffer_size-1\n",
    "        change = value-self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = value\n",
    "        while tree_idx != 0:\n",
    "            tree_idx = (tree_idx-1)//2\n",
    "            self.tree[tree_idx] += change\n",
    "    def get_leaf(self,v):\n",
    "        parent_idx = 0\n",
    "        while True:\n",
    "            left_child_idx = 2*parent_idx+1\n",
    "            right_child_idx = left_child_idx+1\n",
    "            if left_child_idx>=self.tree_size:\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:\n",
    "                if v<=self.tree[left_child_idx]:\n",
    "                    parent_idx = left_child_idx\n",
    "                else:\n",
    "                    v -= self.tree[left_child_idx]\n",
    "                    parent_idx = right_child_idx\n",
    "        data_idx = leaf_idx-self.buffer_size+1\n",
    "        return data_idx,self.tree[leaf_idx]\n",
    "    def sample(self,n,beta):\n",
    "        batch_index = np.zeros(n, dtype=np.long)\n",
    "        IS_weight = torch.zeros(n, dtype=torch.float32)\n",
    "       \n",
    "        total = self.total\n",
    "        pri_seg = total/n\n",
    "        \n",
    "        tree_tensor = torch.tensor(self.tree[-self.buffer_size:], dtype=torch.float32)\n",
    "        min_prob = tree_tensor.min().item() / total\n",
    "        if min_prob == 0:\n",
    "            min_prob = 0.00001\n",
    "        prob_list = []\n",
    "        for i in range(n):\n",
    "            a,b= i*pri_seg,(i+1)*pri_seg\n",
    "            \n",
    "            v = np.random.uniform(a,b)\n",
    "            data_idx,priority = self.get_leaf(v)\n",
    "            prob = priority/total\n",
    "            batch_index[i] = data_idx\n",
    "            prob_list.append(prob/min_prob)\n",
    "        IS_weight = torch.tensor(prob_list, dtype=torch.float32)\n",
    "        IS_weight = torch.pow(IS_weight, -beta)\n",
    "        return batch_index,IS_weight\n",
    "    def add(self,data_idx,value):\n",
    "        tree_idx = data_idx+self.buffer_size-1\n",
    "        self.data[self.data_point] = value\n",
    "        self.update(data_idx,value)\n",
    "        self.data_point += 1\n",
    "        if self.data_point>=self.buffer_size:\n",
    "            self.data_point = 0\n",
    "    @property\n",
    "    def max_priority(self):\n",
    "        return np.max(self.tree[-self.buffer_size:])\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Replaybuffer(object):\n",
    "    def __init__(self,args):\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        \n",
    "        self.buffer = {\n",
    "            'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'action':np.zeros((self.buffer_capacity,1)),\n",
    "            'reward':np.zeros(self.buffer_capacity),\n",
    "            'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'done':np.zeros(self.buffer_capacity)\n",
    "        }\n",
    "        \n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.buffer['state'][self.current_index] = state\n",
    "        self.buffer['action'][self.current_index] = action\n",
    "        self.buffer['reward'][self.current_index] = reward\n",
    "        self.buffer['next_state'][self.current_index] = next_state\n",
    "        self.buffer['done'][self.current_index] = done\n",
    "        self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "        self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "    def sample(self,total_step):\n",
    "        \n",
    "        index = np.random.randint(0,self.current_size,size=self.batch_size)\n",
    "        batch = {}\n",
    "        for key in self.buffer.keys():\n",
    "            if key=='action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.float32)\n",
    "        return batch,None,None\n",
    "class N_Step_ReplayBuffer(object):\n",
    "    def __init__(self,args):\n",
    "            self.batch_size = args.batch_size\n",
    "            self.buffer_capacity = args.buffer_capacity\n",
    "            self.current_size = 0\n",
    "            self.current_index = 0\n",
    "            self.n_step = args.n_steps\n",
    "            self.gamma = args.gamma\n",
    "            self.n_step_deque = deque(maxlen=args.n_steps)\n",
    "            self.buffer = {\n",
    "                'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "                'action':np.zeros((self.buffer_capacity,1)),\n",
    "                'reward':np.zeros(self.buffer_capacity),\n",
    "                'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "                'done':np.zeros(self.buffer_capacity)\n",
    "            }\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.n_step_deque.append((state,action,reward,next_state,done))\n",
    "        if len(self.n_step_deque)>=self.n_step:\n",
    "            state, action, n_steps_reward, next_state, done = self.get_n_steps_transition()\n",
    "            self.buffer['state'][self.current_index] = state\n",
    "            self.buffer['action'][self.current_index] = action\n",
    "            self.buffer['reward'][self.current_index] = n_steps_reward\n",
    "            self.buffer['next_state'][self.current_index] = next_state\n",
    "            self.buffer['done'][self.current_index] = done\n",
    "            self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "            self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "    def get_n_steps_transition(self):\n",
    "        state,action = self.n_step_deque[0][0],self.n_step_deque[0][1]\n",
    "        next_state,done = self.n_step_deque[-1][3],self.n_step_deque[-1][4]\n",
    "        n_steps_reward = 0\n",
    "        for i in reversed(range(self.n_step)):\n",
    "            r,_s,done = self.n_step_deque[i][2:]\n",
    "            n_steps_reward = r + self.gamma*(1-done)*n_steps_reward\n",
    "            if done:\n",
    "                next_state,done = _s,done\n",
    "                break\n",
    "        return state,action,n_steps_reward,next_state,done\n",
    "    def sample(self,total_step):\n",
    "        index = np.random.randint(0,self.current_size,size=self.batch_size)\n",
    "        batch = {}\n",
    "        for key in self.buffer.keys():\n",
    "            if key=='action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][index],dtype=torch.float32)\n",
    "        return batch,None,None\n",
    "    \n",
    "class Prioritized_ReplayBuffer(object):\n",
    "    def __init__(self,args):\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        self.alpha = args.alpha\n",
    "        self.beta = args.beta_init\n",
    "        self.sumtree = SumTree(self.buffer_capacity)\n",
    "        self.beta_increment_per_sampling = args.beta_increment_per_sampling\n",
    "        self.priority_eps = args.priority_eps\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        \n",
    "        self.tree = SumTree(self.buffer_capacity)\n",
    "        \n",
    "        self.buffer = {\n",
    "            'state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'action':np.zeros((self.buffer_capacity,1)),\n",
    "            'reward':np.zeros(self.buffer_capacity),\n",
    "            'next_state':np.zeros((self.buffer_capacity,args.state_dim)),\n",
    "            'done':np.zeros(self.buffer_capacity)\n",
    "        }\n",
    "    def store_transition(self,state,action,reward,next_state,done):\n",
    "        self.buffer['state'][self.current_index] = state\n",
    "        self.buffer['action'][self.current_index] = action\n",
    "        self.buffer['reward'][self.current_index] = reward\n",
    "        self.buffer['next_state'][self.current_index] = next_state\n",
    "        self.buffer['done'][self.current_index] = done\n",
    "        max_priority = self.tree.max_priority if self.current_size else 1.0\n",
    "        self.tree.add(self.current_index,max_priority)\n",
    "        self.current_index  = (self.current_index+1)%self.buffer_capacity\n",
    "        self.current_size = min(self.current_size+1,self.buffer_capacity)\n",
    "    def sample(self,total_step):\n",
    "        batch_index,IS_weight = self.tree.sample(self.batch_size,self.beta)\n",
    "        batch = {}\n",
    "        self.beta = self.beta + (1 - self.beta) * (total_step / self.max_train_steps)\n",
    "        for key in self.buffer.keys():\n",
    "            if key=='action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index],dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index],dtype=torch.float32)\n",
    "        return batch, batch_index, IS_weight \n",
    "    def update_batch_priorities(self, batch_index, td_errors):  # 根据传入的td_error，更新batch_index所对应数据的priorities\n",
    "        priorities = (np.abs(td_errors) + 0.01) ** self.alpha\n",
    "        for index, priority in zip(batch_index, priorities):\n",
    "            self.sum_tree.update(index, priority)\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, args):\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.alpha = args.alpha\n",
    "        self.beta_init = args.beta_init\n",
    "        self.beta = args.beta_init\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.buffer_capacity = args.buffer_capacity\n",
    "        self.sum_tree = SumTree(self.buffer_capacity)\n",
    "        self.n_steps = args.n_steps\n",
    "        self.n_steps_deque = deque(maxlen=self.n_steps)\n",
    "        self.buffer = {'state': np.zeros((self.buffer_capacity, args.state_dim)),\n",
    "                       'action': np.zeros((self.buffer_capacity, 1)),\n",
    "                       'reward': np.zeros(self.buffer_capacity),\n",
    "                       'next_state': np.zeros((self.buffer_capacity, args.state_dim)),\n",
    "                       'done': np.zeros(self.buffer_capacity),\n",
    "                       }\n",
    "        self.current_size = 0\n",
    "        self.current_index = 0\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.device = args.device\n",
    "        self.support = torch.linspace(self.v_min, self.v_max, self.num_atoms).to(self.device)\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.n_steps_deque.append((state, action, reward, next_state, done))\n",
    "        if len(self.n_steps_deque) == self.n_steps:\n",
    "            state, action, n_steps_reward, next_state, done = self.get_n_steps_transition()\n",
    "            self.buffer['state'][self.current_index] = state\n",
    "            self.buffer['action'][self.current_index] = action\n",
    "            self.buffer['reward'][self.current_index] = n_steps_reward\n",
    "            self.buffer['next_state'][self.current_index] = next_state\n",
    "            self.buffer['done'][self.current_index] = done\n",
    "            max_priority = self.sum_tree.max_priority if self.current_size else 1.0\n",
    "            self.sum_tree.update(self.current_index, max_priority)\n",
    "            self.current_index = (self.current_index + 1) % self.buffer_capacity\n",
    "            self.current_size = min(self.current_size + 1, self.buffer_capacity)\n",
    "    def get_n_steps_transition(self):\n",
    "        state, action = self.n_steps_deque[0][0], self.n_steps_deque[0][1]\n",
    "        next_state, done = self.n_steps_deque[-1][3], self.n_steps_deque[-1][4]\n",
    "        n_steps_reward = 0\n",
    "        for i in reversed(range(self.n_steps)):\n",
    "            r, _s, done = self.n_steps_deque[i][2:]\n",
    "            n_steps_reward = r + self.gamma * (1 - done) * n_steps_reward\n",
    "            if done:\n",
    "                next_state, done = _s, done\n",
    "                break\n",
    "        return state, action, n_steps_reward, next_state, done\n",
    "    def sample(self, total_step):\n",
    "        batch_index, IS_weight = self.sum_tree.sample(self.batch_size, self.beta)\n",
    "        batch = {}\n",
    "        self.beta = self.beta_init + (1 - self.beta_init) * (total_step / self.max_train_steps)\n",
    "        for key in self.buffer.keys():\n",
    "            if key == 'action':\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index], dtype=torch.long)\n",
    "            else:\n",
    "                batch[key] = torch.tensor(self.buffer[key][batch_index], dtype=torch.float32)\n",
    "        return batch, batch_index, IS_weight\n",
    "    def update_batch_priorities(self, batch_index, td_errors): \n",
    "        values = (np.abs(td_errors) + 1e-6) ** self.alpha\n",
    "        \n",
    "        for index, value in zip(batch_index, values):\n",
    "            \n",
    "            self.sum_tree.update(index, value)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "class DQN(object):\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.action_dim = args.action_dim\n",
    "        self.state_dim = args.state_dim\n",
    "        self.lr = args.lr\n",
    "        self.dqn_net = NetWork(args).to(args.device)\n",
    "        self.target_net =deepcopy(self.dqn_net)\n",
    "        self.optimizer = torch.optim.Adam(self.dqn_net.parameters(),lr=args.lr)\n",
    "        self.update_counter = 0\n",
    "        self.target_update_freq = args.target_update_freq\n",
    "        self.max_train_steps = args.max_train_steps\n",
    "        self.gamma = args.gamma\n",
    "        self.beta = args.beta_init\n",
    "        self.beta_increment_per_sampling = args.beta_increment_per_sampling\n",
    "        self.priority_eps = args.priority_eps\n",
    "        self.n_step = args.n_steps\n",
    "        self.num_atoms = args.num_atoms\n",
    "        self.v_min = args.v_min\n",
    "        self.v_max = args.v_max\n",
    "        self.batch_size = args.batch_size\n",
    "        self.replay_buffer = ReplayBuffer(args)\n",
    "        self.support = torch.linspace(self.v_min,self.v_max,self.num_atoms).to(args.device)\n",
    "    def choose_action(self,state,epsilon):\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            state = torch.unsqueeze(torch.tensor(state,dtype=torch.float),0).to(self.args.device)\n",
    "       \n",
    "            q = self.dqn_net(state)\n",
    "            if np.random.uniform() > epsilon:\n",
    "                action = q.argmax(dim=-1).item()\n",
    "            else:\n",
    "                action = np.random.randint(0, self.action_dim)\n",
    "        return action\n",
    "    def learn(self,total_step):\n",
    "        gamma = self.gamma**self.n_step\n",
    "        batch, idx, IS_weight = self.replay_buffer.sample(total_step)\n",
    "        IS_weight = torch.FloatTensor(IS_weight.reshape(-1,1)).to(self.args.device)\n",
    "        element_loss,td_error  = self.compute_loss(batch,total_step,gamma)\n",
    "        loss  = torch.mean(element_loss*IS_weight)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.dqn_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_for_prior = td_error.detach().cpu().numpy()  \n",
    "        new_priority = self.priority_eps + loss_for_prior\n",
    "        self.replay_buffer.update_batch_priorities(idx, new_priority)\n",
    "        self.dqn_net.reset_noise()\n",
    "        self.target_net.reset_noise()\n",
    "        \n",
    "        self.update_counter += 1\n",
    "        if self.update_counter%self.target_update_freq==0:\n",
    "            self.target_net.load_state_dict(self.dqn_net.state_dict())\n",
    "        self.update_lr(total_step)\n",
    "    def update_lr(self,total_steps):\n",
    "        lr_now = 0.9 * self.lr * (1 - total_steps / self.max_train_steps) + 0.1 * self.lr\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr_now\n",
    "    def compute_loss(self,batch,total_step,gamma):\n",
    "       \n",
    "        state = torch.FloatTensor(batch[\"state\"]).to(self.args.device)\n",
    "        next_state = torch.FloatTensor(batch[\"next_state\"]).to(self.args.device)\n",
    "        action = torch.LongTensor(batch[\"action\"]).to(self.args.device)\n",
    "        reward = torch.FloatTensor(batch[\"reward\"].reshape(-1, 1)).to(self.args.device)\n",
    "        done = torch.FloatTensor(batch[\"done\"].reshape(-1, 1)).to(self.args.device)\n",
    "        delta_z = float(self.v_max - self.v_min)/(self.num_atoms-1)\n",
    "        action = action.squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn_net(next_state).argmax(1)\n",
    "            next_dist = self.target_net.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.num_atoms, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.num_atoms)\n",
    "                .to(self.args.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.args.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn_net.dist(state)\n",
    "       \n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        \n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "       \n",
    "        Q_s_a = (dist[range(self.batch_size), action] * self.support).sum(dim=1)  \n",
    "\n",
    "# 目标分布对应的期望 Q(s,a)，即投影后的分布\n",
    "        Q_target = (proj_dist * self.support).sum(dim=1)  # [batch_size]\n",
    "\n",
    "# 绝对 TD 误差，用于更新 PER\n",
    "        td_errors = (Q_target - Q_s_a).abs().detach()     # [batch_size]\n",
    "        #print(f\"td_errors shape: {td_errors.shape}, td_errors dtype: {td_errors.dtype}\")\n",
    "        return elementwise_loss,td_errors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env=LunarLander-v3\n",
      "state_dim=8\n",
      "action_dim=4\n",
      "episode_limit=1000\n",
      "device=cuda:0\n",
      "total_steps:0 \t evaluate_reward:-115.02922986528642 \t epsilon：0\n",
      "total_steps:1000 \t evaluate_reward:-620.197239036679 \t epsilon：0\n",
      "total_steps:2000 \t evaluate_reward:-260.10190032545705 \t epsilon：0\n",
      "total_steps:3000 \t evaluate_reward:-478.35231195943305 \t epsilon：0\n",
      "total_steps:4000 \t evaluate_reward:-386.1971691503624 \t epsilon：0\n",
      "total_steps:5000 \t evaluate_reward:-316.96642725195755 \t epsilon：0\n",
      "total_steps:6000 \t evaluate_reward:-256.3774207086031 \t epsilon：0\n",
      "total_steps:7000 \t evaluate_reward:-70.40187010626921 \t epsilon：0\n",
      "total_steps:8000 \t evaluate_reward:-102.26345992775676 \t epsilon：0\n",
      "total_steps:9000 \t evaluate_reward:-176.73868174138155 \t epsilon：0\n",
      "total_steps:10000 \t evaluate_reward:57.003470914967124 \t epsilon：0\n",
      "total_steps:11000 \t evaluate_reward:-223.12129674423485 \t epsilon：0\n",
      "total_steps:12000 \t evaluate_reward:-152.57316638542514 \t epsilon：0\n",
      "total_steps:13000 \t evaluate_reward:-116.11231307568703 \t epsilon：0\n",
      "total_steps:14000 \t evaluate_reward:-107.11644246989341 \t epsilon：0\n",
      "total_steps:15000 \t evaluate_reward:-96.90381695603428 \t epsilon：0\n",
      "total_steps:16000 \t evaluate_reward:-36.62009055927548 \t epsilon：0\n",
      "total_steps:17000 \t evaluate_reward:15.694233926848227 \t epsilon：0\n",
      "total_steps:18000 \t evaluate_reward:-72.32226286933262 \t epsilon：0\n",
      "total_steps:19000 \t evaluate_reward:-101.40846182128594 \t epsilon：0\n",
      "total_steps:20000 \t evaluate_reward:-68.48738219209221 \t epsilon：0\n",
      "total_steps:21000 \t evaluate_reward:-183.78230504945034 \t epsilon：0\n",
      "total_steps:22000 \t evaluate_reward:-95.13029092699112 \t epsilon：0\n",
      "total_steps:23000 \t evaluate_reward:25.999376149456427 \t epsilon：0\n",
      "total_steps:24000 \t evaluate_reward:-11.329534896368315 \t epsilon：0\n",
      "total_steps:25000 \t evaluate_reward:-114.74172565662279 \t epsilon：0\n",
      "total_steps:26000 \t evaluate_reward:-138.1353446853949 \t epsilon：0\n",
      "total_steps:27000 \t evaluate_reward:-108.17467938491215 \t epsilon：0\n",
      "total_steps:28000 \t evaluate_reward:36.84610768970783 \t epsilon：0\n",
      "total_steps:29000 \t evaluate_reward:-92.68798653876009 \t epsilon：0\n",
      "total_steps:30000 \t evaluate_reward:-68.67305038946013 \t epsilon：0\n",
      "total_steps:31000 \t evaluate_reward:-93.5179059319662 \t epsilon：0\n",
      "total_steps:32000 \t evaluate_reward:-90.8175522734414 \t epsilon：0\n",
      "total_steps:33000 \t evaluate_reward:-82.69288550106944 \t epsilon：0\n",
      "total_steps:34000 \t evaluate_reward:-97.8008194180873 \t epsilon：0\n",
      "total_steps:35000 \t evaluate_reward:-87.67925766160111 \t epsilon：0\n",
      "total_steps:36000 \t evaluate_reward:-133.40083673768078 \t epsilon：0\n",
      "total_steps:37000 \t evaluate_reward:-121.58844652151338 \t epsilon：0\n",
      "total_steps:38000 \t evaluate_reward:-82.08297312566425 \t epsilon：0\n",
      "total_steps:39000 \t evaluate_reward:-101.36784740614554 \t epsilon：0\n",
      "total_steps:40000 \t evaluate_reward:-108.25480951596627 \t epsilon：0\n",
      "total_steps:41000 \t evaluate_reward:-122.34850153959617 \t epsilon：0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m,\u001b[32m20\u001b[39m,\u001b[32m50\u001b[39m,\u001b[32m70\u001b[39m,\u001b[32m100\u001b[39m]:\n\u001b[32m    152\u001b[39m     runner = Runner(args=args, env_name=env_names[env_index], number=\u001b[32m1\u001b[39m, seed=seed)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m                 \u001b[38;5;28mself\u001b[39m.agent.learn(\u001b[38;5;28mself\u001b[39m.total_steps)\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total_steps % \u001b[38;5;28mself\u001b[39m.args.evaluate_freq == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Save reward\u001b[39;00m\n\u001b[32m     85\u001b[39m np.save(\u001b[33m'\u001b[39m\u001b[33m./data_train/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_env_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_number_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_seed_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.algorithm, \u001b[38;5;28mself\u001b[39m.env_name, \u001b[38;5;28mself\u001b[39m.number, \u001b[38;5;28mself\u001b[39m.seed), np.array(\u001b[38;5;28mself\u001b[39m.evaluate_rewards))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mRunner.evaluate_policy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     95\u001b[39m episode_reward = \u001b[32m0\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     next_state, reward, done,  truncated ,_= \u001b[38;5;28mself\u001b[39m.env_evaluate.step(action)\n\u001b[32m     99\u001b[39m     done = done \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mDQN.choose_action\u001b[39m\u001b[34m(self, state, epsilon)\u001b[39m\n\u001b[32m     31\u001b[39m q = \u001b[38;5;28mself\u001b[39m.dqn_net(state)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.random.uniform() > epsilon:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     action = \u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     action = np.random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.action_dim)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "class Runner:\n",
    "    def __init__(self,args,env_name,number,seed):\n",
    "        self.args = args\n",
    "        self.env_name = env_name\n",
    "        self.number = number\n",
    "        self.seed = seed\n",
    "\n",
    "        self.env = gym.make(env_name)  # When training the policy, we need to build an environment\n",
    "        self.env_evaluate = gym.make(env_name)  # When evaluating the policy, we need to rebuild an environment\n",
    "        #self.env.seed(seed)\n",
    "        self.env.reset(seed=seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "        self.env_evaluate.reset(seed=seed)\n",
    "        self.env_evaluate.action_space.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed=seed)\n",
    "        torch.cuda.manual_seed_all(seed=seed)\n",
    "\n",
    "        self.args.state_dim = self.env.observation_space.shape[0]\n",
    "        self.args.action_dim = self.env.action_space.n\n",
    "        self.args.episode_limit = self.env._max_episode_steps  # Maximum number of steps per episode\n",
    "        self.device = args.device\n",
    "        self.n_steps = args.n_steps\n",
    "        print(\"env={}\".format(self.env_name))\n",
    "        print(\"state_dim={}\".format(self.args.state_dim))\n",
    "        print(\"action_dim={}\".format(self.args.action_dim))\n",
    "        print(\"episode_limit={}\".format(self.args.episode_limit))\n",
    "        print(\"device={}\".format(self.device))\n",
    "        self.agent = DQN(args)\n",
    "\n",
    "        self.algorithm = 'Rainbow-DQN'\n",
    "        self.writer = SummaryWriter(log_dir='runs/DQN/{}_env_{}_number_{}_seed_{}'.format(self.algorithm, env_name, number, seed))\n",
    "\n",
    "        self.evaluate_num = 0  # Record the number of evaluations\n",
    "        self.evaluate_rewards = []  # Record the rewards during the evaluating\n",
    "        self.total_steps = 0  # Record the total steps during the training\n",
    "        self.epsilon = self.args.epsilon_init\n",
    "        self.epsilon_min = self.args.epsilon_min\n",
    "        self.epsilon_decay = (self.args.epsilon_init - self.args.epsilon_min) / self.args.epsilon_decay_steps\n",
    "        #self.replay_buffer = ReplayBuffer(args)  # Initialize the replay buffer\n",
    "        \n",
    "\n",
    "        self.beta = self.args.beta_init\n",
    "        self.writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(self.args).items()])),\n",
    "    )\n",
    "    def run(self, ):\n",
    "        self.evaluate_policy()\n",
    "        #total_reward = 0\n",
    "        while self.total_steps < self.args.max_train_steps:\n",
    "            state = self.env.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_steps = 0\n",
    "            #total_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, epsilon=self.epsilon)\n",
    "                next_state, reward, done,  truncated,_= self.env.step(action) \n",
    "                done = done or truncated\n",
    "                episode_steps += 1\n",
    "                self.total_steps += 1\n",
    "                #total_reward += reward\n",
    "                self.agent.replay_buffer.store_transition(state, action, reward, next_state, done)  # Store the transition\n",
    "                \n",
    "                \n",
    "                self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon - self.epsilon_decay > self.epsilon_min else self.epsilon_min\n",
    "\n",
    "               \n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if episode_steps%4==0:\n",
    "                    if self.agent.replay_buffer.current_size >= self.args.batch_size:\n",
    "                    \n",
    "                    \n",
    "                        self.agent.learn(self.total_steps)\n",
    "\n",
    "                if self.total_steps % self.args.evaluate_freq == 0:\n",
    "                    \n",
    "                    self.evaluate_policy()\n",
    "        # Save reward\n",
    "        np.save('./data_train/{}_env_{}_number_{}_seed_{}.npy'.format(self.algorithm, self.env_name, self.number, self.seed), np.array(self.evaluate_rewards))\n",
    "        \n",
    "\n",
    "    def evaluate_policy(self, ):\n",
    "        evaluate_reward = 0\n",
    "        self.agent.dqn_net.eval()\n",
    "        for _ in range(self.args.evaluate_times):\n",
    "            state = self.env_evaluate.reset()\n",
    "            state = state[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = self.agent.choose_action(state, epsilon=0)\n",
    "                next_state, reward, done,  truncated ,_= self.env_evaluate.step(action)\n",
    "                done = done or truncated\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            evaluate_reward += episode_reward\n",
    "        self.agent.dqn_net.train()\n",
    "        evaluate_reward /= self.args.evaluate_times\n",
    "        self.evaluate_rewards.append(evaluate_reward)\n",
    "        print(\"total_steps:{} \\t evaluate_reward:{} \\t epsilon：{}\".format(self.total_steps, evaluate_reward, self.epsilon))\n",
    "        self.writer.add_scalar('step_rewards_{}'.format(self.env_name), evaluate_reward, global_step=self.total_steps)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"Hyperparameter Setting for DQN\")\n",
    "    parser.add_argument(\"--device\", type=str, default='cuda:0', help=\"device\")\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=int(1e5), help=\" Maximum number of training steps\")\n",
    "    parser.add_argument(\"--evaluate_freq\", type=float, default=1e3, help=\"Evaluate the policy every 'evaluate_freq' steps\")\n",
    "    parser.add_argument(\"--evaluate_times\", type=float, default=3, help=\"Evaluate times\")\n",
    "\n",
    "    parser.add_argument(\"--buffer_capacity\", type=int, default=int(1e5), help=\"The maximum replay-buffer capacity \")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size\")\n",
    "    parser.add_argument(\"--hidden_dim\", type=int, default=512, help=\"The number of neurons in hidden layers of the neural network\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0000625, help=\"Learning rate of actor\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"Discount factor\")\n",
    "    parser.add_argument(\"--epsilon_init\", type=float, default=0, help=\"Initial epsilon\")\n",
    "    parser.add_argument(\"--epsilon_min\", type=float, default=0, help=\"Minimum epsilon\")\n",
    "    parser.add_argument(\"--epsilon_decay_steps\", type=int, default=int(1e5), help=\"How many steps before the epsilon decays to the minimum\")\n",
    "    parser.add_argument(\"--tau\", type=float, default=0.005, help=\"soft update the target network\")\n",
    "    parser.add_argument(\"--use_soft_update\", type=bool, default=False, help=\"Whether to use soft update\")\n",
    "    parser.add_argument(\"--target_update_freq\", type=int, default=100, help=\"Update frequency of the target network(hard update)\")\n",
    "    parser.add_argument(\"--n_steps\", type=int, default=3, help=\"n_steps\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.6, help=\"PER parameter\")\n",
    "    parser.add_argument(\"--beta_init\", type=float, default=0.4, help=\"Important sampling parameter in PER\")\n",
    "    parser.add_argument(\"--use_lr_decay\", type=bool, default=True, help=\"Learning rate Decay\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=10.0, help=\"Gradient clip\")\n",
    "    parser.add_argument(\"--num_atoms\", type=int, default=51, help=\"Number of atoms in distributional DQN\")\n",
    "    parser.add_argument(\"--v_min\", type=float, default=-10, help=\"Minimum value of the support\")\n",
    "    parser.add_argument(\"--v_max\", type=float, default=10, help=\"Maximum value of the support\")\n",
    "    parser.add_argument(\"--priority_eps\", type=float, default=1e-6, help=\"Priority eps\")\n",
    "    parser.add_argument(\"--beta_increment_per_sampling\", type=float, default=0.001, help=\"Increment of beta per sampling\")\n",
    "\n",
    "    parser.add_argument(\"--use_double\", type=bool, default=True, help=\"Whether to use double Q-learning\")\n",
    "    parser.add_argument(\"--use_dueling\", type=bool, default=True, help=\"Whether to use dueling network\")\n",
    "    parser.add_argument(\"--use_noisy\", type=bool, default=True, help=\"Whether to use noisy network\")\n",
    "    parser.add_argument(\"--use_per\", type=bool, default=True, help=\"Whether to use PER\")\n",
    "    parser.add_argument(\"--use_n_steps\", type=bool, default=True, help=\"Whether to use n_steps Q-learning\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    #args = parser.parse_known_args()[0]\n",
    "   \n",
    "    env_names = ['CartPole-v1', 'LunarLander-v3','PongNoFrameskip-v4']\n",
    "    env_index = 1\n",
    "    seed = 0\n",
    "    for seed in [0,20,50,70,100]:\n",
    "        runner = Runner(args=args, env_name=env_names[env_index], number=1, seed=seed)\n",
    "        runner.run()\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r_l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
